# Design
- [x] more readable config
- [ ] re-write ./cofig/ folder


# Attention
- [x] autoregressive masking in basic attention

- [ ] flash attention
- [ ] sliding window attention 


# PE
- [ ] RoPE


# Transformer (with Vision)
- [ ] EvoFormer
- [ ] GraphFormer
- [ ] DiT


# Fine-tuning
- [ ] LoRA
- [ ] PEFT Adapter


# Training
- [ ] Simple training setup
