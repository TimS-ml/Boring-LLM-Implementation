# Ref:
- paper: Attention Is All You Need http://arxiv.org/abs/1706.03762v7
- [xmu-xiaoma666/External-Attention-pytorch: üçÄ Pytorch implementation of various Attention Mechanisms, MLP, Re-parameter, Convolution, which is helpful to further understand papers.‚≠ê‚≠ê‚≠ê](https://github.com/xmu-xiaoma666/External-Attention-pytorch)
- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
- [lucidrains/x-transformers: A simple but complete full-attention transformer with a set of promising experimental features from various papers](https://github.com/lucidrains/x-transformers)


