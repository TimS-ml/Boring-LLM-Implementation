"""
Universal Pretraining Wrapper for Transformers

This module implements the Universal Pretraining approach from:
"Universal Pretraining" by Peter Bloem
https://arxiv.org/abs/2506.20057

The key idea is to pretrain transformers on synthetic data generated by a
"Turing machine" (LSTM/GRU), which is periodically reset to generate diverse
training data. This approach enables pretraining without requiring large
datasets.
"""

# https://arxiv.org/abs/2506.20057
# Peter Bloem

from __future__ import annotations
from functools import partial
from random import randrange, uniform

import torch
from torch import nn, cat, tensor, randperm
from torch.nn import LSTM, GRU, Module

from x_transformers.x_transformers import (
    TransformerWrapper,
    AutoregressiveWrapper
)

# functions

def exists(v):
    """
    Check if a value is not None.

    Args:
        v: Any value to check

    Returns:
        bool: True if v is not None, False otherwise
    """
    return v is not None

def default(v, d):
    """
    Return the value v if it exists, otherwise return default value d.

    Args:
        v: The value to check
        d: The default value to return if v is None

    Returns:
        Either v or d depending on whether v exists
    """
    return v if exists(v) else d

def divisible_by(num, den):
    """
    Check if num is divisible by den.

    Args:
        num (int): Numerator
        den (int): Denominator

    Returns:
        bool: True if num is evenly divisible by den, False otherwise
    """
    return (num % den) == 0

# random sequences, mixture of random and constant (unsure why constant is needed)

def random_sequences(
    num_tokens,
    seq_len,
    num_samples_random,
    num_samples_constant,
    shuffle = True,
    device = None
):
    """
    Generate a mixture of random and constant token sequences.

    Creates two types of sequences:
    1. Random sequences: Each position has a random token
    2. Constant sequences: All positions have the same random token

    The mixture of random and constant sequences provides diversity in the
    training data for universal pretraining.

    Args:
        num_tokens (int): Vocabulary size (number of possible token values)
        seq_len (int): Length of each sequence
        num_samples_random (int): Number of random sequences to generate
        num_samples_constant (int): Number of constant sequences to generate
        shuffle (bool): Whether to shuffle the combined sequences. Defaults to True.
        device (str or torch.device, optional): Device to place the sequences on

    Returns:
        Tensor: Combined sequences of shape (num_samples_random + num_samples_constant, seq_len)

    Note:
        At least one of num_samples_random or num_samples_constant must be > 0
    """
    assert num_samples_random > 0 or num_samples_constant > 0

    # Generate completely random sequences
    rand_seq = torch.randint(0, num_tokens, (num_samples_random, seq_len))
    # Generate sequences where all tokens are the same (constant)
    const_seq = torch.full((num_samples_constant, seq_len), randrange(num_tokens))

    # Concatenate both types of sequences
    all_seq = cat((rand_seq, const_seq))

    # Move to specified device if provided
    if exists(device):
        all_seq = all_seq.to(device)

    if not shuffle:
        return all_seq

    # shuffle with randperm

    # Randomly permute the order of sequences
    rand_indices = randperm(all_seq.shape[0], device = all_seq.device)
    return all_seq[rand_indices]

# synthetic data generator

class SyntheticDataGenerator(Module):
    """
    Synthetic Data Generator using LSTM or GRU.

    This module serves as a "Turing machine" that generates synthetic sequences
    for universal pretraining. It uses an LSTM or GRU to generate coherent
    sequences that the transformer learns to predict.

    The generator can be periodically reset to provide diverse training data
    across different "tasks" represented by different random initializations.

    Args:
        dim (int): Embedding dimension
        num_tokens (int): Vocabulary size
        max_seq_len (int): Maximum sequence length. Defaults to 512.
        hidden_size (int, optional): Hidden size for LSTM/GRU. Defaults to dim.
        use_gru (bool): If True, use GRU instead of LSTM. Defaults to False.
        network_klass (callable, optional): Custom network class. If None, uses LSTM or GRU.

    Attributes:
        max_seq_len: Maximum sequence length
        embed: Token embedding layer
        net: LSTM or GRU network
        to_logits: Output projection to vocabulary logits
    """
    def __init__(
        self,
        dim,
        num_tokens,
        max_seq_len = 512,
        hidden_size = None,
        use_gru = False,
        network_klass = None
    ):
        super().__init__()

        self.max_seq_len = max_seq_len

        # Token embedding layer
        self.embed = nn.Embedding(num_tokens, dim)

        # Use provided hidden size or default to embedding dimension
        hidden_size = default(hidden_size, dim)

        # Create LSTM or GRU with batch_first=True for easier handling
        default_network_klass = partial(LSTM if not use_gru else GRU, batch_first = True)
        network_klass = default(network_klass, default_network_klass)

        # Recurrent network (LSTM or GRU)
        self.net = network_klass(dim, hidden_size)

        # Project hidden states to vocabulary logits
        self.to_logits = nn.Linear(dim, num_tokens, bias = False)

        # Apply custom initialization
        self.apply(self.init_)

    def reset_(self):
        """
        Reset all parameters of the generator.

        This is called periodically during universal pretraining to create
        diverse synthetic data from different "Turing machines". Resetting
        the parameters creates a new task for the transformer to learn.
        """
        # Reset parameters of all modules that support it
        for m in self.modules():
            if hasattr(m, 'reset_parameters'):
                m.reset_parameters()

        # Re-apply custom initialization
        self.apply(self.init_)

    @torch.no_grad()
    def init_(self, m):
        """
        Custom initialization for linear layers.

        Scales weights by a random factor between 0 and 1.1, following
        the paper's approach to create diverse initial conditions.

        Args:
            m (nn.Module): Module to initialize
        """
        if isinstance(m, nn.Linear):
            # Scale weights by random factor (he scales the lstm weights from 0 to 1.1)
            m.weight *= uniform(0., 1.1)

    @torch.inference_mode()
    @torch.compile
    def generate(
        self,
        length,
        seed = None,
        condition = None,
        temperature = 1e-4 # he uses a near greedy temperature
    ):
        """
        Generate a sequence of tokens autoregressively.

        Uses the LSTM/GRU to generate a coherent sequence token by token.
        The generation can be conditioned on a seed and/or condition sequence.

        Args:
            length (int): Number of tokens to generate
            seed (Tensor, optional): Seed sequence to start generation
            condition (Tensor, optional): Conditioning sequence
            temperature (float): Sampling temperature. Lower values make sampling
                more greedy. Defaults to 1e-4 (near-greedy).

        Returns:
            Tensor: Generated sequence trimmed to max_seq_len

        Note:
            At least one of seed or condition must be provided
        """
        assert exists(seed) or exists(condition)
        # Combine seed and condition (filter out None values)
        prefix = [*filter(exists, (seed, condition))]
        seq_len = self.max_seq_len

        # Concatenate prefix sequences
        seq = torch.cat(prefix, dim = -1)

        # Initial input is the full prefix
        net_input = seq
        hiddens = None

        # Generate tokens autoregressively
        for _ in range(length):

            # Get logits and updated hidden states
            logits, hiddens = self.forward(net_input, hiddens)

            # Use only the last token's logits for prediction
            last_logit = logits[:, -1]
            # Apply temperature and softmax to get probabilities
            prob = (last_logit / temperature).softmax(dim = -1)

            # Sample next token from the probability distribution
            sampled = torch.multinomial(prob, 1)
            # Use only the sampled token as input for next step
            net_input = sampled

            # Append sampled token to sequence
            seq = torch.cat((seq, sampled), dim = -1)

        # Return only the last max_seq_len tokens
        return seq[:, -seq_len:]

    def forward(
        self,
        input,
        hiddens = None
    ):
        """
        Forward pass through the LSTM/GRU generator.

        Args:
            input (Tensor): Input token IDs of shape (batch, seq_len)
            hiddens (tuple, optional): Hidden states from previous forward pass.
                For LSTM: (h, c). For GRU: h. Defaults to None.

        Returns:
            tuple: (logits, hidden)
                - logits (Tensor): Output logits of shape (batch, seq_len, num_tokens)
                - hidden (tuple or Tensor): Updated hidden states
        """
        # Embed input tokens
        tokens = self.embed(input)

        # Pass through LSTM/GRU
        embed, hidden = self.net(tokens, hiddens)

        # Project to vocabulary logits
        logits = self.to_logits(embed)

        return logits, hidden

# classes

class UniversalPretrainWrapper(Module):
    """
    Universal Pretraining Wrapper for Transformer models.

    Implements the universal pretraining algorithm from "Universal Pretraining"
    by Peter Bloem. The key idea is to train a transformer on synthetic data
    generated by a "Turing machine" (LSTM/GRU), which is periodically reset
    to create diverse training tasks.

    The algorithm maintains a buffer of sequences that are "enriched" by the
    Turing machine. The transformer learns to predict these enriched sequences,
    which provides a form of pretraining that doesn't require large datasets.

    Args:
        model (TransformerWrapper): The transformer model to train. Must be causal.
        data_generator (SyntheticDataGenerator or Module, optional): The synthetic
            data generator (Turing machine). If None, creates a default LSTM-based
            generator.
        buffer_size (int, optional): Size of the data buffer. Defaults to batch_size * 20.
        num_reset (int): Number of sequences in buffer to reset each step. Must be
            divisible by 2. Defaults to 20.
        batch_size (int): Batch size for training. Defaults to 32.
        seq_len (int): Sequence length. Defaults to 512.
        seed_length (int): Length of seed sequences for generation. Defaults to 8.
        reset_turing_machine_every (int): Reset the Turing machine every N steps.
            If 0, never reset. Defaults to 0.
        keep_buffer_on_cpu (bool): If True, keep the data buffer on CPU to save
            GPU memory. Defaults to False.

    Attributes:
        model: The transformer model being trained
        ar_wrapped: Autoregressive wrapper around the model
        data_generator: The Turing machine for generating synthetic data
        synth_data_buffer: Buffer of sequences (on GPU or CPU)
        step: Training step counter
    """
    def __init__(
        self,
        model: TransformerWrapper,
        data_generator: SyntheticDataGenerator | Module | None = None,
        buffer_size = None,
        num_reset = 20,
        batch_size = 32,
        seq_len = 512,
        seed_length = 8,
        reset_turing_machine_every = 0,
        keep_buffer_on_cpu = False
    ):
        super().__init__()

        self.model = model
        # Wrap model for autoregressive training
        self.ar_wrapped = AutoregressiveWrapper(model)

        # Ensure model is causal (for autoregressive generation)
        assert model.attn_layers.causal

        # Extract model parameters
        num_tokens = model.num_tokens
        dim = model.attn_layers.dim

        # Create default data generator if not provided
        if not exists(data_generator):
            data_generator = SyntheticDataGenerator(
                num_tokens = num_tokens,
                dim = dim,
                max_seq_len = seq_len
            )

        # How often to reset the Turing machine (0 = never)
        self.reset_turing_machine_every = reset_turing_machine_every

        self.seq_len = seq_len
        self.data_generator = data_generator

        self.seed_length = seed_length
        self.batch_size = batch_size

        # Buffer must be larger than batch size to allow sampling
        buffer_size = default(buffer_size, batch_size * 20)
        assert buffer_size > batch_size, f'data buffer size must be greater than batch size'

        # num_reset must be even for equal split between random and constant sequences
        assert divisible_by(num_reset, 2)
        self.num_reset = num_reset

        self.buffer_size = buffer_size

        # Partial function for generating random sequences
        self.random_sequences_fn = partial(random_sequences, num_tokens, seq_len)

        # Initialize buffer with half random, half constant sequences
        init_data_buffer = self.random_sequences_fn(buffer_size // 2, buffer_size // 2)

        # Store buffer on CPU or GPU depending on memory constraints
        if keep_buffer_on_cpu:
            self.synth_data_buffer = init_data_buffer
        else:
            self.register_buffer('synth_data_buffer', init_data_buffer)

        # Track training steps
        self.register_buffer('step', tensor(0))

    @property
    def device(self):
        """
        Get the device of the module.

        Returns:
            torch.device: Device where the model is located
        """
        return self.step.device

    def get_rand_sequences_from_buffer(self, size = None):
        """
        Sample random sequences from the data buffer.

        Args:
            size (int, optional): Number of sequences to sample.
                Defaults to self.batch_size.

        Returns:
            Tensor: Randomly sampled sequences of shape (size, seq_len)
        """
        size = default(size, self.batch_size)
        # Generate random indices and sample from buffer
        rand_indices = randperm(self.buffer_size, device = self.device)[:size]
        return self.synth_data_buffer[rand_indices]

    def forward(self):
        """
        Execute one step of the universal pretraining algorithm.

        Implements Algorithm 1 from the paper:
        1. Sample condition sequences from buffer
        2. Sample and crop seed sequences from buffer
        3. Generate enriched sequences using the Turing machine
        4. Optionally reset the Turing machine
        5. Replace some buffer sequences with fresh random sequences
        6. Update buffer with enriched sequences
        7. Sample final training batch and compute loss

        Returns:
            Tensor: Loss from autoregressive prediction on sampled sequences
        """
        # following algorithm 1.

        # Sample sequences from buffer to be enriched
        conditions = self.get_rand_sequences_from_buffer()

        # get seeds, which appears to be random sequences with random crops of seed length

        # Sample sequences and extract random crops as seeds
        seeds = self.get_rand_sequences_from_buffer()

        # Create random crops of seed_length from each sequence
        seq_arange = torch.arange(self.seed_length)
        rand_offset = torch.randint(0, self.seq_len - self.seed_length, (self.batch_size,))
        seq_start_pos = rand_offset[:, None] + seq_arange

        # Index into seeds to extract the random crops
        batch_arange = torch.arange(self.batch_size, device = self.device)[:, None]
        seeds = seeds[batch_arange, seq_start_pos]

        # seed, condition to turing machine

        # Use Turing machine to generate enriched sequences
        generated = self.data_generator.generate(
            self.seq_len,
            condition = conditions.to(self.device),
            seed = seeds.to(self.device)
        )

        # Increment step counter
        self.step.add_(1)

        # maybe reset turing machine

        # Periodically reset Turing machine to create new tasks
        if self.reset_turing_machine_every > 0 and divisible_by(self.step.item(), self.reset_turing_machine_every):
            self.data_generator.reset_()

        # reset

        # Replace some buffer sequences with fresh random sequences
        # This maintains diversity and prevents the buffer from becoming stale
        if self.num_reset > 0:
            buffer_to_reset = self.get_rand_sequences_from_buffer(self.num_reset)

            with torch.no_grad():
                # Generate fresh random sequences (half random, half constant)
                reset_sequences = self.random_sequences_fn(self.num_reset // 2, self.num_reset // 2, device = self.device)
                buffer_to_reset.copy_(reset_sequences)

        # place "enriched" random generated sequences back

        # Update buffer in-place with enriched sequences from Turing machine
        with torch.no_grad():
            conditions.copy_(generated)

        # sample yet again according to pseudocode

        # Sample final training batch (may overlap with previous samples)
        data = self.get_rand_sequences_from_buffer().to(self.device)

        # Compute autoregressive loss on the sampled sequences
        return self.ar_wrapped(data)
