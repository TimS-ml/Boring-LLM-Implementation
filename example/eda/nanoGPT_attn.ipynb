{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref:\n",
    "- https://github.com/TimS-ml/nanoGPT\n",
    "- https://youtu.be/kCc8FmEb1nY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from boring_llm_base.constants import PROJECT_HOME_DIR\n",
    "import sys; sys.path.append(str(PROJECT_HOME_DIR)); os.chdir(PROJECT_HOME_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boring_utils.utils import (\n",
    "    cprint, \n",
    "    tprint, \n",
    "    get_device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> device:\u001b[0m\n",
      "device(type='mps')\n"
     ]
    }
   ],
   "source": [
    "DEV = True \n",
    "\n",
    "if not DEV:\n",
    "    batch_size = 64  # how many independent sequences will we process in parallel?\n",
    "    block_size = 256  # what is the maximum context length for predictions?\n",
    "    # max_iters = 5000\n",
    "    max_iters = 4000\n",
    "    eval_interval = 500\n",
    "    learning_rate = 3e-4\n",
    "    eval_iters = 200\n",
    "    n_embd = 384\n",
    "    n_embed = n_embd\n",
    "    n_head = 6\n",
    "    n_layer = 6\n",
    "    dropout = 0.2\n",
    "\n",
    "else:\n",
    "    batch_size = 32\n",
    "    block_size = 8\n",
    "    # max_iters = 1000\n",
    "    max_iters = 100\n",
    "    eval_interval = 500\n",
    "    learning_rate = 3e-4\n",
    "    eval_iters = 200\n",
    "    n_embd = 32\n",
    "    n_embed = n_embd\n",
    "    n_head = 4\n",
    "    n_layer = 4\n",
    "    dropout = 0.2\n",
    "\n",
    "device = get_device()\n",
    "# vocab_size = len(set(text))\n",
    "cprint(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('DATA_DIR', './data/')\n",
    "data_dir = os.path.join(data_dir, 'enwik8')\n",
    "\n",
    "# # NOTE: only read enwik8 first 10M bytes\n",
    "# with gzip.open(os.path.join(data_dir, 'enwik8.gz')) as file:\n",
    "#     text = file.read(int(10e6)).decode('utf-8')\n",
    "\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    stoi = meta['stoi']\n",
    "    itos = meta['itos']\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Meta file {meta_path} not found\")\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bin_path = os.path.join(data_dir, 'train.bin')\n",
    "val_bin_path = os.path.join(data_dir, 'val.bin')\n",
    "\n",
    "# train_tensor = torch.tensor(encode(data), dtype=torch.long) # convert to tensor\n",
    "\n",
    "# torch.long is just an alias for torch.int64\n",
    "# load the binary data\n",
    "train_data = np.fromfile(train_bin_path, dtype=np.uint16)\n",
    "val_data = np.fromfile(val_bin_path, dtype=np.uint16)\n",
    "\n",
    "# convert to pytorch tensors\n",
    "train_data = torch.from_numpy(train_data.astype(np.int64))\n",
    "val_data = torch.from_numpy(val_data.astype(np.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = int(block_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # single sample\n",
    "        ix = torch.randint(\n",
    "            len(self.data) - self.block_size - 1, (1,)\n",
    "        )\n",
    "        full_seq = self.data[ix:ix + self.block_size + 1]\n",
    "        x = full_seq[:-1]\n",
    "        y = full_seq[1:]\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.block_size\n",
    "\n",
    "\n",
    "train_dataset = TextSamplerDataset(train_data, block_size)\n",
    "val_dataset = TextSamplerDataset(val_data, block_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (embedding): Embedding(2102, 2102)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        embedding_dim = vocab_size\n",
    "        # embedding_dim = 128\n",
    "        # each token is represented by a one-hot vector\n",
    "        # directly reads off the logits for the next token from the embedding table\n",
    "        # for example: 24 will reads off the 24th column of the embedding table\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is (batch_size, block_size)\n",
    "        logits = self.embedding(idx)  # B, T, C: (batch_size, block_size, embedding_dim)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # (batch_size * block_size, embedding_dim)\n",
    "            targets = targets.view(-1)  # (batch_size * block_size)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 8.1311, val loss 8.1381\n",
      "step 99: train loss 8.0976, val loss 8.0863\n"
     ]
    }
   ],
   "source": [
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "train_iter = cycle(train_loader)\n",
    "val_iter = cycle(val_loader)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # Eval logic\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for _, (x, y) in zip(range(eval_iters), val_iter):\n",
    "                _, loss = model(x, y)\n",
    "                val_losses.append(loss.item())\n",
    "            val_loss = np.mean(val_losses)\n",
    "            \n",
    "            train_losses = []\n",
    "            for _, (x, y) in zip(range(eval_iters), train_loader):\n",
    "                _, loss = model(x, y)\n",
    "                train_losses.append(loss.item())\n",
    "            train_loss = np.mean(train_losses)\n",
    "            \n",
    "            print(f\"step {iter}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "        model.train()\n",
    "\n",
    "    # Training logic\n",
    "    x, y = next(train_iter)  # replace get_batch\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
