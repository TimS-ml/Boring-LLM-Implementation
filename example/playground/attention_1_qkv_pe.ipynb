{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref:\n",
    "- https://github.com/TimS-ml/nanoGPT\n",
    "- https://youtu.be/kCc8FmEb1nY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from boring_llm_base.constants import PROJECT_HOME_DIR\n",
    "import sys; sys.path.append(str(PROJECT_HOME_DIR)); os.chdir(PROJECT_HOME_DIR)\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from jaxtyping import Float, Bool\n",
    "\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from boring_utils.utils import (\n",
    "    cprint, \n",
    "    tprint, \n",
    "    get_device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> device:\u001b[0m device(type='mps')\n"
     ]
    }
   ],
   "source": [
    "# from boring_nn.attention.config import AttentionConfig\n",
    "# cfg = AttentionConfig()\n",
    "# cprint(cfg)\n",
    "\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # time steps (seq length, context window)\n",
    "n_embed = 36    # channels (embedding dim)\n",
    "\n",
    "t_enc, t_dec = 10, block_size  # encoder/decoder sequence lengths \n",
    "n_head = 6\n",
    "assert n_embed % n_head == 0\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "max_iters = 100\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "\n",
    "device = get_device()\n",
    "# vocab_size = len(set(text))\n",
    "cprint(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('DATA_DIR', './data/')\n",
    "data_dir = os.path.join(data_dir, 'enwik8')\n",
    "\n",
    "# # NOTE: only read enwik8 first 10M bytes\n",
    "# with gzip.open(os.path.join(data_dir, 'enwik8.gz')) as file:\n",
    "#     text = file.read(int(10e6)).decode('utf-8')\n",
    "\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    stoi = meta['stoi']\n",
    "    itos = meta['itos']\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Meta file {meta_path} not found\")\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) \n",
    "\n",
    "train_bin_path = os.path.join(data_dir, 'train.bin')\n",
    "val_bin_path = os.path.join(data_dir, 'val.bin')\n",
    "\n",
    "# train_tensor = torch.tensor(encode(data), dtype=torch.long) # convert to tensor\n",
    "\n",
    "# torch.long is just an alias for torch.int64\n",
    "# load the binary data\n",
    "train_data = np.fromfile(train_bin_path, dtype=np.uint16)\n",
    "val_data = np.fromfile(val_bin_path, dtype=np.uint16)\n",
    "\n",
    "# convert to pytorch tensors\n",
    "train_data = torch.from_numpy(train_data.astype(np.int64))\n",
    "val_data = torch.from_numpy(val_data.astype(np.int64))\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = int(block_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # single sample\n",
    "        ix = torch.randint(\n",
    "            len(self.data) - self.block_size - 1, (1,)\n",
    "        )\n",
    "        full_seq = self.data[ix:ix + self.block_size + 1]\n",
    "        x = full_seq[:-1]\n",
    "        y = full_seq[1:]\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.block_size\n",
    "\n",
    "\n",
    "train_dataset = TextSamplerDataset(train_data, block_size)\n",
    "val_dataset = TextSamplerDataset(val_data, block_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (embedding): Embedding(2102, 2102)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        embedding_dim = vocab_size\n",
    "        # embedding_dim = 128\n",
    "        # each token is represented by a one-hot vector\n",
    "        # directly reads off the logits for the next token from the embedding table\n",
    "        # for example: 24 will reads off the 24th column of the embedding table\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is (batch_size, block_size)\n",
    "        logits = self.embedding(idx)  # B, T, C: (batch_size, block_size, embedding_dim)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # (batch_size * block_size, embedding_dim)\n",
    "            targets = targets.view(-1)  # (batch_size * block_size)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "train_iter = cycle(train_loader)\n",
    "val_iter = cycle(val_loader)\n",
    "\n",
    "def train(\n",
    "        model: nn.Module = model,\n",
    "        train_iter: DataLoader = train_iter,\n",
    "        val_iter: DataLoader = val_iter,\n",
    "        eval_iters: int = eval_iters,\n",
    "        max_iters: int = max_iters,\n",
    "        eval_interval: int = eval_interval,\n",
    "    ):\n",
    "    for iter in range(max_iters):\n",
    "        # Eval logic\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_losses = []\n",
    "                for _, (x, y) in zip(range(eval_iters), val_iter):\n",
    "                    _, loss = model(x, y)\n",
    "                    val_losses.append(loss.item())\n",
    "                val_loss = np.mean(val_losses)\n",
    "\n",
    "                train_losses = []\n",
    "                for _, (x, y) in zip(range(eval_iters), train_iter):\n",
    "                    _, loss = model(x, y)\n",
    "                    train_losses.append(loss.item())\n",
    "                train_loss = np.mean(train_losses)\n",
    "\n",
    "                print(f\"step {iter}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "            model.train()\n",
    "\n",
    "        # Training logic\n",
    "        x, y = next(train_iter)  # replace get_batch\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Zone\n",
    "\n",
    "From nanoGPT:\n",
    "\n",
    "```python\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\n",
      "******************** qkv_normal -> allclose ********************\u001b[0m\n",
      "passed\n",
      "\u001b[33m\n",
      "******************** qkv_normal -> shape ********************\u001b[0m\n",
      "passed\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(batch_size, block_size, n_embed)\n",
    "\n",
    "def qkv_normal(\n",
    "    x: Float[Tensor, \"batch seq_len embedding_dim\"],\n",
    "    num_heads: int = n_head\n",
    "):\n",
    "    batch, seq_len, dim = x.shape\n",
    "    head_dim = dim // num_heads\n",
    "    \n",
    "    # Fixed weights, for testing `assert torch.allclose`\n",
    "    fixed_weight = torch.randn(dim, dim)\n",
    "\n",
    "    # Separated QKV (x-transformer style)\n",
    "    def impl1(x):\n",
    "        to_q = nn.Linear(dim, dim, bias=False)\n",
    "        to_kv = nn.Linear(dim, dim * 2, bias=False)\n",
    "        to_q.weight.data = fixed_weight.clone()\n",
    "        to_kv.weight.data = torch.cat([fixed_weight, fixed_weight]).clone()\n",
    "\n",
    "        q = to_q(x)\n",
    "        k, v = to_kv(x).chunk(2, dim=-1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'batch seq_len (num_heads head_dim) -> batch num_heads seq_len head_dim', num_heads = num_heads), (q, k, v))\n",
    "        return q, k, v\n",
    "\n",
    "    # Unified QKV (lit-gpt style)\n",
    "    def impl2(x):\n",
    "        to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        to_qkv.weight.data = torch.cat([fixed_weight] * 3).clone() \n",
    "\n",
    "        # Split into q,k,v and reshape to add head dimension\n",
    "        q, k, v  = to_qkv(x).split(n_embed, dim=2)\n",
    "        # Permute to get (batch, num_heads, seq_len, head_dim)\n",
    "        q, k, v = map(\n",
    "            lambda t: t.view(batch, -1, num_heads, head_dim).transpose(1, 2),\n",
    "            (q, k, v)\n",
    "        )\n",
    "\n",
    "        return q, k, v\n",
    "\n",
    "    # nanoGPT style\n",
    "    def impl3(x):\n",
    "        to_q = nn.Linear(dim, head_dim, bias=False)\n",
    "        to_k = nn.Linear(dim, head_dim, bias=False)\n",
    "        to_v = nn.Linear(dim, head_dim, bias=False)\n",
    "        to_q.weight.data = fixed_weight.clone()\n",
    "        to_k.weight.data = fixed_weight.clone()\n",
    "        to_v.weight.data = fixed_weight.clone()\n",
    "        \n",
    "        k = to_k(x)\n",
    "        q = to_q(x)\n",
    "        v = to_v(x)\n",
    "        \n",
    "        q = q.view(batch, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "        k = k.view(batch, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "        v = v.view(batch, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "        return q, k, v    \n",
    "\n",
    "    # Compare results\n",
    "    q1, k1, v1 = impl1(x)\n",
    "    q2, k2, v2 = impl2(x)\n",
    "    q3, k3, v3 = impl3(x)\n",
    "    \n",
    "    tprint('allclose', sep='*')\n",
    "    assert torch.allclose(q1, q2, rtol=1e-4)\n",
    "    assert torch.allclose(k1, k2, rtol=1e-4)\n",
    "    assert torch.allclose(v1, v2, rtol=1e-4)\n",
    "    assert torch.allclose(q1, q3, rtol=1e-4)\n",
    "    assert torch.allclose(k1, k3, rtol=1e-4)\n",
    "    assert torch.allclose(v1, v3, rtol=1e-4)\n",
    "    print('passed')\n",
    "\n",
    "    tprint('shape', sep='*')\n",
    "    try:\n",
    "        assert q1.shape == q2.shape\n",
    "        assert k1.shape == k2.shape\n",
    "        assert v1.shape == v2.shape\n",
    "        assert q1.shape == q3.shape\n",
    "        assert k1.shape == k3.shape\n",
    "        assert v1.shape == v3.shape\n",
    "        print('passed')\n",
    "    except Exception as e:\n",
    "        cprint(q1.shape, k1.shape, v1.shape)\n",
    "        cprint(q2.shape, k2.shape, v2.shape)\n",
    "        cprint(q3.shape, k3.shape, v3.shape)\n",
    "        raise e\n",
    "\n",
    "\n",
    "qkv_normal(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped QKV\n",
    "\n",
    "From lit-gpt:\n",
    "```\n",
    "to use multi-head attention (MHA), set this to `n_head` (default)\n",
    "to use multi-query attention (MQA), set this to 1\n",
    "to use grouped-query attention (GQA), set this to a value in between\n",
    "Example with `n_head=4`\n",
    "┌───┐┌───┐┌───┐┌───┐     ┌───┐    ┌───┐             ┌───┐\n",
    "│ v ││ v ││ v ││ v │     │ v │    │ v │             │ v │\n",
    "└───┘└───┘└───┘└───┘     └───┘    └───┘             └───┘\n",
    "  │    │    │    │         │        │                 │\n",
    "┌───┐┌───┐┌───┐┌───┐     ┌───┐    ┌───┐             ┌───┐\n",
    "│ k ││ k ││ k ││ k │     │ k │    │ k │             │ k │\n",
    "└───┘└───┘└───┘└───┘     └───┘    └───┘             └───┘\n",
    "  │    │    │    │      ┌──┴──┐  ┌──┴──┐      ┌────┬──┴─┬────┐\n",
    "┌───┐┌───┐┌───┐┌───┐  ┌───┐┌───┐┌───┐┌───┐  ┌───┐┌───┐┌───┐┌───┐\n",
    "│ q ││ q ││ q ││ q │  │ q ││ q ││ q ││ q │  │ q ││ q ││ q ││ q │\n",
    "└───┘└───┘└───┘└───┘  └───┘└───┘└───┘└───┘  └───┘└───┘└───┘└───┘\n",
    "◀──────────────────▶  ◀──────────────────▶  ◀──────────────────▶\n",
    "        MHA                    GQA                   MQA\n",
    "  n_query_groups=4       n_query_groups=2      n_query_groups=1\n",
    "\n",
    "credit https://arxiv.org/pdf/2305.13245.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      "==================== <module> -> MHA ====================\u001b[0m\n",
      "\u001b[33m\n",
      "******************** qkv_grouped -> shape ********************\u001b[0m\n",
      "passed\n",
      "\u001b[35m\n",
      "==================== <module> -> MQA ====================\u001b[0m\n",
      "\u001b[33m\n",
      "******************** qkv_grouped -> shape ********************\u001b[0m\n",
      "passed\n",
      "\u001b[35m\n",
      "==================== <module> -> GQA ====================\u001b[0m\n",
      "\u001b[33m\n",
      "******************** qkv_grouped -> shape ********************\u001b[0m\n",
      "passed\n"
     ]
    }
   ],
   "source": [
    "def qkv_grouped(\n",
    "    x: Float[Tensor, \"batch seq_len embedding_dim\"],\n",
    "    num_heads: int = n_head,  # Number of heads in Q\n",
    "    n_query_groups: int = 2   # Number of heads in KV\n",
    "):\n",
    "    \"\"\"\n",
    "    Number of heads in Q is unchanged.\n",
    "    Number of heads in KV is n_query_groups.\n",
    "    \"\"\"\n",
    "\n",
    "    batch, seq_len, dim = x.shape\n",
    "    head_dim = dim // num_heads\n",
    "    \n",
    "    # TODO: Fixed weights for testing\n",
    "    # q_weight = torch.randn(dim, dim)\n",
    "    # k_weight = torch.randn(dim, head_dim * n_query_groups)\n",
    "    # v_weight = torch.randn(dim, head_dim * n_query_groups)\n",
    "\n",
    "    # 1. x-transformer style implementation (Separated QKV + GQA)\n",
    "    # in x-transformers, n_query_groups is called kv_heads\n",
    "    def impl1(x):\n",
    "        to_q = nn.Linear(dim, dim, bias=False)  # or head_dim * num_heads\n",
    "        to_kv = nn.Linear(dim, head_dim * n_query_groups * 2, bias=False)\n",
    "        # to_q.weight.data = q_weight.clone()\n",
    "        # to_kv.weight.data = torch.cat([k_weight, v_weight], dim=1)\n",
    "\n",
    "        q = to_q(x)\n",
    "        k, v = to_kv(x).chunk(2, dim=-1)\n",
    "\n",
    "        # Handle grouped-query attention\n",
    "        if n_query_groups == 1:  # MQA case\n",
    "            # method 1\n",
    "            # k, v = tuple(rearrange(t, 'b n (h d) -> b h n d', h=kv_heads) for t in (k, v))\n",
    "            # k, v = tuple(repeat(t, 'b h n d -> b (r h) n d', r=num_heads // kv_heads) for t in (k, v))\n",
    "\n",
    "            # method 2\n",
    "            # k, v = map(\n",
    "            #     lambda t: repeat(\n",
    "            #         rearrange(t, 'b n (h d) -> b h n d', h=kv_heads),\n",
    "            #         'b h n d -> b (r h) n d',\n",
    "            #         r=num_heads // kv_heads\n",
    "            #     ),\n",
    "            #     (k, v)\n",
    "            # )\n",
    "\n",
    "            # method 3\n",
    "            # k, v = tuple(repeat(t, 'b n (h d) -> b (r h) n d', h=n_query_groups, r=num_heads // n_query_groups) for t in (k, v))\n",
    "            k, v = tuple(repeat(\n",
    "                    t, 'batch seq_len (kv_heads head_dim) -> batch (q_per_kv kv_heads) seq_len head_dim', \n",
    "                    kv_heads=n_query_groups, \n",
    "                    q_per_kv=num_heads // n_query_groups\n",
    "                ) for t in (k, v))\n",
    "        elif n_query_groups < num_heads:  # GQA case\n",
    "            k, v = tuple(repeat(\n",
    "                    t, 'batch seq_len (kv_heads head_dim) -> batch (q_per_kv kv_heads) seq_len head_dim', \n",
    "                    kv_heads=n_query_groups, \n",
    "                    q_per_kv=num_heads // n_query_groups\n",
    "                ) for t in (k, v))\n",
    "        else:  # MHA case\n",
    "            k, v = map(lambda t: rearrange(t, 'batch seq_len (num_heads head_dim) -> batch num_heads seq_len head_dim', num_heads = num_heads), (k, v))\n",
    "\n",
    "        q = rearrange(q, 'batch seq_len (num_heads head_dim) -> batch num_heads seq_len head_dim', num_heads = num_heads)\n",
    "        return q, k, v\n",
    "\n",
    "    # 2. lit-gpt style implementation (Unified QKV + GQA)\n",
    "    def impl2(x):\n",
    "        # Calculate number of Q per KV group\n",
    "        q_per_kv = num_heads // n_query_groups\n",
    "        total_qkv = q_per_kv + 2  # Each group has q_per_kv queries + 1 key + 1 value\n",
    "        # qkv_dim = (num_heads + 2 * n_query_groups) * head_dim\n",
    "        qkv_dim = (total_qkv * n_query_groups) * head_dim  # save value\n",
    "\n",
    "        to_qkv = nn.Linear(dim, qkv_dim, bias=False)\n",
    "        # qkv_weight = torch.cat([\n",
    "        #     q_weight,\n",
    "        #     k_weight.repeat(q_per_kv, 1),\n",
    "        #     v_weight.repeat(q_per_kv, 1)\n",
    "        # ], dim=1)\n",
    "        # to_qkv.weight.data = qkv_weight.clone()\n",
    "\n",
    "        qkv = to_qkv(x)\n",
    "        qkv = qkv.view(batch, seq_len, n_query_groups, total_qkv, head_dim)\n",
    "        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)\n",
    "        \n",
    "        # Split Q, K, V\n",
    "        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)\n",
    "        \n",
    "        # Handle K, V expansion for MQA/GQA\n",
    "        if n_query_groups != num_heads:\n",
    "            k = k.expand(batch, n_query_groups, q_per_kv, seq_len, head_dim)\n",
    "            v = v.expand(batch, n_query_groups, q_per_kv, seq_len, head_dim)\n",
    "        \n",
    "        # Reshape to final shape\n",
    "        q = q.reshape(batch, -1, seq_len, head_dim)  # (B, nh_q, T, hs)\n",
    "        k = k.reshape(batch, -1, seq_len, head_dim)  # (B, nh_k, T, hs)\n",
    "        v = v.reshape(batch, -1, seq_len, head_dim)  # (B, nh_v, T, hs)\n",
    "        \n",
    "        return q, k, v\n",
    "\n",
    "    # Validate implementations\n",
    "    q1, k1, v1 = impl1(x)\n",
    "    q2, k2, v2 = impl2(x)\n",
    "    \n",
    "    # tprint('allclose', sep='*')\n",
    "    # assert torch.allclose(q1, q2, rtol=1e-4)\n",
    "    # assert torch.allclose(k1, k2, rtol=1e-4)\n",
    "    # assert torch.allclose(v1, v2, rtol=1e-4)\n",
    "    # print('passed')\n",
    "\n",
    "    tprint('shape', sep='*')\n",
    "    try:\n",
    "        assert q1.shape == q2.shape\n",
    "        assert k1.shape == k2.shape\n",
    "        assert v1.shape == v2.shape\n",
    "        print('passed')\n",
    "    except Exception as e:\n",
    "        cprint(q1.shape, k1.shape, v1.shape)\n",
    "        cprint(q2.shape, k2.shape, v2.shape)\n",
    "        raise e\n",
    "\n",
    "\n",
    "# Test different GQA configurations\n",
    "x = torch.randn(batch_size, block_size, n_embed)\n",
    "\n",
    "# Test standard multi-head attention\n",
    "tprint('MHA')\n",
    "qkv_grouped(x, num_heads=n_head, n_query_groups=n_head)\n",
    "\n",
    "# Test MQA (1 KV head)\n",
    "tprint('MQA')\n",
    "qkv_grouped(x, num_heads=n_head, n_query_groups=1)\n",
    "\n",
    "# Test GQA (e.g. 8 heads, 2 groups)\n",
    "tprint('GQA')\n",
    "qkv_grouped(x, num_heads=n_head, n_query_groups=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PE\n",
    "\n",
    "Sin/Cos PE:\n",
    "\n",
    "$$\n",
    "PE(\\text{position}, 2i) = \\sin\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(\\text{position}, 2i+1) = \\cos\\bigg( \\frac{ \\text{position} }{10000^\\frac{2i}{d_{model}}} \\bigg)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_normal(\n",
    "    x: Float[Tensor, \"batch seq_len embedding_dim\"],\n",
    "    num_heads: int = n_head\n",
    "):\n",
    "    batch, seq_len, dim = x.shape\n",
    "    head_dim = dim // num_heads\n",
    "    \n",
    "    # Fixed weights for testing\n",
    "    # fixed_weight = torch.randn(seq_len, dim)\n",
    "\n",
    "    # x-transformer style (using sin-cos)\n",
    "    def impl1(x):\n",
    "        # pe = fixed_weight.clone()\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        return x + pe\n",
    "\n",
    "    # nanoGPT style (simple positional embedding)\n",
    "    def impl2(x):\n",
    "        position_embeddings = nn.Embedding(seq_len, dim)\n",
    "        # position_embeddings.weight.data = fixed_weight.clone()\n",
    "        \n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        position_embeds = position_embeddings(positions)\n",
    "        return x + position_embeds\n",
    "\n",
    "    # Compare results\n",
    "    out1 = impl1(x)\n",
    "    out2 = impl2(x)\n",
    "    \n",
    "    tprint('shape', sep='*')\n",
    "    cprint(out1.shape, out2.shape)\n",
    "    assert out1.shape == out2.shape\n",
    "    print('passed')\n",
    "\n",
    "\n",
    "# positional_encoding_normal(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE\n",
    "\n",
    "Ref:\n",
    "- https://huggingface.co/jinaai/xlm-roberta-flash-implementation/blob/main/rotary.py\n",
    "- https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/layers/rotary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_RoPE(\n",
    "    x: Float[Tensor, \"batch seq_len embedding_dim\"],\n",
    "    num_heads: int = n_head\n",
    "):\n",
    "    batch, seq_len, dim = x.shape\n",
    "    head_dim = dim // num_heads\n",
    "    \n",
    "    # Fixed weights for testing\n",
    "    fixed_weight = torch.randn(seq_len, dim)\n",
    "\n",
    "    # x-transformer style (using sin-cos)\n",
    "    def impl1(x):\n",
    "        pe = fixed_weight.clone()\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        return x + pe\n",
    "\n",
    "    # lit-gpt style (using RoPE)\n",
    "    def impl2(x):\n",
    "        # Generate sin/cos for RoPE\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2) / head_dim))\n",
    "        t = torch.arange(seq_len)\n",
    "        freqs = torch.einsum('i,j->ij', t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos().view(1, seq_len, 1, head_dim)\n",
    "        sin = emb.sin().view(1, seq_len, 1, head_dim)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        x_reshaped = x.view(batch, seq_len, num_heads, head_dim)\n",
    "        x_rope = apply_rope(x_reshaped, cos, sin)\n",
    "        return x_rope.view(batch, seq_len, -1)\n",
    "\n",
    "    # Compare results\n",
    "    out1 = impl1(x)\n",
    "    out2 = impl2(x)\n",
    "    \n",
    "    tprint('shape', sep='*')\n",
    "    cprint(out1.shape, out2.shape)\n",
    "    assert out1.shape == out2.shape\n",
    "    print('passed')\n",
    "\n",
    "\n",
    "# positional_encoding_RoPE(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
