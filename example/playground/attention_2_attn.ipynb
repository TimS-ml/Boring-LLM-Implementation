{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref:\n",
    "- https://github.com/TimS-ml/nanoGPT\n",
    "- https://youtu.be/kCc8FmEb1nY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from boring_llm_base.constants import PROJECT_HOME_DIR\n",
    "import sys; sys.path.append(str(PROJECT_HOME_DIR)); os.chdir(PROJECT_HOME_DIR)\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from jaxtyping import Float, Bool\n",
    "\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from boring_utils.utils import (\n",
    "    cprint, \n",
    "    tprint, \n",
    "    get_device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> device:\u001b[0m\n",
      "device(type='mps')\n"
     ]
    }
   ],
   "source": [
    "# from boring_nn.attention.config import AttentionConfig\n",
    "# cfg = AttentionConfig()\n",
    "# cprint(cfg)\n",
    "\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # time steps (seq length, context window)\n",
    "n_embed = 36    # channels (embedding dim)\n",
    "\n",
    "t_enc, t_dec = 10, block_size  # encoder/decoder sequence lengths \n",
    "n_head = 6\n",
    "assert n_embed % n_head == 0\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "max_iters = 100\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "\n",
    "device = get_device()\n",
    "# vocab_size = len(set(text))\n",
    "cprint(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('DATA_DIR', './data/')\n",
    "data_dir = os.path.join(data_dir, 'enwik8')\n",
    "\n",
    "# # NOTE: only read enwik8 first 10M bytes\n",
    "# with gzip.open(os.path.join(data_dir, 'enwik8.gz')) as file:\n",
    "#     text = file.read(int(10e6)).decode('utf-8')\n",
    "\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    stoi = meta['stoi']\n",
    "    itos = meta['itos']\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Meta file {meta_path} not found\")\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) \n",
    "\n",
    "train_bin_path = os.path.join(data_dir, 'train.bin')\n",
    "val_bin_path = os.path.join(data_dir, 'val.bin')\n",
    "\n",
    "# train_tensor = torch.tensor(encode(data), dtype=torch.long) # convert to tensor\n",
    "\n",
    "# torch.long is just an alias for torch.int64\n",
    "# load the binary data\n",
    "train_data = np.fromfile(train_bin_path, dtype=np.uint16)\n",
    "val_data = np.fromfile(val_bin_path, dtype=np.uint16)\n",
    "\n",
    "# convert to pytorch tensors\n",
    "train_data = torch.from_numpy(train_data.astype(np.int64))\n",
    "val_data = torch.from_numpy(val_data.astype(np.int64))\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = int(block_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # single sample\n",
    "        ix = torch.randint(\n",
    "            len(self.data) - self.block_size - 1, (1,)\n",
    "        )\n",
    "        full_seq = self.data[ix:ix + self.block_size + 1]\n",
    "        x = full_seq[:-1]\n",
    "        y = full_seq[1:]\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.block_size\n",
    "\n",
    "\n",
    "train_dataset = TextSamplerDataset(train_data, block_size)\n",
    "val_dataset = TextSamplerDataset(val_data, block_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (embedding): Embedding(2102, 2102)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        embedding_dim = vocab_size\n",
    "        # embedding_dim = 128\n",
    "        # each token is represented by a one-hot vector\n",
    "        # directly reads off the logits for the next token from the embedding table\n",
    "        # for example: 24 will reads off the 24th column of the embedding table\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is (batch_size, block_size)\n",
    "        logits = self.embedding(idx)  # B, T, C: (batch_size, block_size, embedding_dim)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # (batch_size * block_size, embedding_dim)\n",
    "            targets = targets.view(-1)  # (batch_size * block_size)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "train_iter = cycle(train_loader)\n",
    "val_iter = cycle(val_loader)\n",
    "\n",
    "def train(\n",
    "        model: nn.Module = model,\n",
    "        train_iter: DataLoader = train_iter,\n",
    "        val_iter: DataLoader = val_iter,\n",
    "        eval_iters: int = eval_iters,\n",
    "        max_iters: int = max_iters,\n",
    "        eval_interval: int = eval_interval,\n",
    "    ):\n",
    "    for iter in range(max_iters):\n",
    "        # Eval logic\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_losses = []\n",
    "                for _, (x, y) in zip(range(eval_iters), val_iter):\n",
    "                    _, loss = model(x, y)\n",
    "                    val_losses.append(loss.item())\n",
    "                val_loss = np.mean(val_losses)\n",
    "\n",
    "                train_losses = []\n",
    "                for _, (x, y) in zip(range(eval_iters), train_iter):\n",
    "                    _, loss = model(x, y)\n",
    "                    train_losses.append(loss.item())\n",
    "                train_loss = np.mean(train_losses)\n",
    "\n",
    "                print(f\"step {iter}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "            model.train()\n",
    "\n",
    "        # Training logic\n",
    "        x, y = next(train_iter)  # replace get_batch\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Zone\n",
    "\n",
    "From nanoGPT:\n",
    "\n",
    "```python\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_normal(\n",
    "    q: Float[Tensor, \"batch num_heads seq_len head_dim\"],\n",
    "    k: Float[Tensor, \"batch num_heads seq_len head_dim\"],\n",
    "    v: Float[Tensor, \"batch num_heads seq_len head_dim\"],\n",
    "    mask: Optional[Bool[Tensor, \"batch seq_len seq_len\"]] = None\n",
    "):\n",
    "    batch, num_heads, seq_len, head_dim = q.shape\n",
    "    scale = 1.0 / math.sqrt(head_dim)\n",
    "\n",
    "    # x-transformer style\n",
    "    def impl1(q, k, v):\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * scale\n",
    "        \n",
    "        if exists(mask):\n",
    "            sim = sim.masked_fill(~mask, float('-inf'))\n",
    "            \n",
    "        attn = F.softmax(sim, dim=-1)\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        return out, attn\n",
    "\n",
    "    # lit-gpt style (use torch.nn.functional)\n",
    "    def impl2(q, k, v):\n",
    "        if hasattr(F, 'scaled_dot_product_attention'):\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q.transpose(1, 2),  # (b, seq, head, dim)\n",
    "                k.transpose(1, 2),\n",
    "                v.transpose(1, 2),\n",
    "                attn_mask=mask,\n",
    "                dropout_p=0.0,\n",
    "                is_causal=mask is None\n",
    "            )\n",
    "            return out.transpose(1, 2), None  # 返回None作为attn weights\n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        if exists(mask):\n",
    "            attn = attn.masked_fill(~mask, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        return out, attn\n",
    "\n",
    "    # nanoGPT style\n",
    "    def impl3(q, k, v):\n",
    "        att = (q @ k.transpose(-2, -1)) * scale\n",
    "        if exists(mask):\n",
    "            att = att.masked_fill(~mask, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        out = att @ v\n",
    "        return out, att\n",
    "\n",
    "    out1, attn1 = impl1(q, k, v)\n",
    "    out2, attn2 = impl2(q, k, v)\n",
    "    out3, attn3 = impl3(q, k, v)\n",
    "    \n",
    "    tprint('shape', sep='*')\n",
    "    cprint(out1.shape)\n",
    "    assert out1.shape == out2.shape == out3.shape\n",
    "    if attn2 is not None:  # if not using SDPA\n",
    "        assert attn1.shape == attn2.shape == attn3.shape\n",
    "    print('passed')\n",
    "    \n",
    "    tprint('allclose', sep='*')\n",
    "    assert torch.allclose(out1, out3, rtol=1e-4)\n",
    "    if attn2 is not None:  # if not using SDPA\n",
    "        assert torch.allclose(out1, out2, rtol=1e-4)\n",
    "        assert torch.allclose(attn1, attn3, rtol=1e-4)\n",
    "    print('passed')\n",
    "\n",
    "    return out1, attn1\n",
    "\n",
    "\n",
    "# attention_normal(q, k, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
