{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref:\n",
    "- https://github.com/TimS-ml/nanoGPT\n",
    "- https://youtu.be/kCc8FmEb1nY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from boring_llm_base.constants import PROJECT_HOME_DIR\n",
    "import sys; sys.path.append(str(PROJECT_HOME_DIR)); os.chdir(PROJECT_HOME_DIR)\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from jaxtyping import Float, Bool\n",
    "\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from boring_utils.utils import (\n",
    "    cprint, \n",
    "    tprint, \n",
    "    get_device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> device:\u001b[0m\n",
      "device(type='mps')\n"
     ]
    }
   ],
   "source": [
    "# from boring_nn.attention.config import AttentionConfig\n",
    "# cfg = AttentionConfig()\n",
    "# cprint(cfg)\n",
    "\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # time steps (seq length, context window)\n",
    "n_embed = 36    # channels (embedding dim)\n",
    "\n",
    "t_enc, t_dec = 10, block_size  # encoder/decoder sequence lengths \n",
    "n_head = 6\n",
    "assert n_embed % n_head == 0\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "max_iters = 100\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "\n",
    "device = get_device()\n",
    "# vocab_size = len(set(text))\n",
    "cprint(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('DATA_DIR', './data/')\n",
    "data_dir = os.path.join(data_dir, 'enwik8')\n",
    "\n",
    "# # NOTE: only read enwik8 first 10M bytes\n",
    "# with gzip.open(os.path.join(data_dir, 'enwik8.gz')) as file:\n",
    "#     text = file.read(int(10e6)).decode('utf-8')\n",
    "\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    stoi = meta['stoi']\n",
    "    itos = meta['itos']\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Meta file {meta_path} not found\")\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) \n",
    "\n",
    "train_bin_path = os.path.join(data_dir, 'train.bin')\n",
    "val_bin_path = os.path.join(data_dir, 'val.bin')\n",
    "\n",
    "# train_tensor = torch.tensor(encode(data), dtype=torch.long) # convert to tensor\n",
    "\n",
    "# torch.long is just an alias for torch.int64\n",
    "# load the binary data\n",
    "train_data = np.fromfile(train_bin_path, dtype=np.uint16)\n",
    "val_data = np.fromfile(val_bin_path, dtype=np.uint16)\n",
    "\n",
    "# convert to pytorch tensors\n",
    "train_data = torch.from_numpy(train_data.astype(np.int64))\n",
    "val_data = torch.from_numpy(val_data.astype(np.int64))\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = int(block_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # single sample\n",
    "        ix = torch.randint(\n",
    "            len(self.data) - self.block_size - 1, (1,)\n",
    "        )\n",
    "        full_seq = self.data[ix:ix + self.block_size + 1]\n",
    "        x = full_seq[:-1]\n",
    "        y = full_seq[1:]\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.block_size\n",
    "\n",
    "\n",
    "train_dataset = TextSamplerDataset(train_data, block_size)\n",
    "val_dataset = TextSamplerDataset(val_data, block_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (embedding): Embedding(2102, 2102)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        embedding_dim = vocab_size\n",
    "        # embedding_dim = 128\n",
    "        # each token is represented by a one-hot vector\n",
    "        # directly reads off the logits for the next token from the embedding table\n",
    "        # for example: 24 will reads off the 24th column of the embedding table\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is (batch_size, block_size)\n",
    "        logits = self.embedding(idx)  # B, T, C: (batch_size, block_size, embedding_dim)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # (batch_size * block_size, embedding_dim)\n",
    "            targets = targets.view(-1)  # (batch_size * block_size)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "train_iter = cycle(train_loader)\n",
    "val_iter = cycle(val_loader)\n",
    "\n",
    "def train(\n",
    "        model: nn.Module = model,\n",
    "        train_iter: DataLoader = train_iter,\n",
    "        val_iter: DataLoader = val_iter,\n",
    "        eval_iters: int = eval_iters,\n",
    "        max_iters: int = max_iters,\n",
    "        eval_interval: int = eval_interval,\n",
    "    ):\n",
    "    for iter in range(max_iters):\n",
    "        # Eval logic\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_losses = []\n",
    "                for _, (x, y) in zip(range(eval_iters), val_iter):\n",
    "                    _, loss = model(x, y)\n",
    "                    val_losses.append(loss.item())\n",
    "                val_loss = np.mean(val_losses)\n",
    "\n",
    "                train_losses = []\n",
    "                for _, (x, y) in zip(range(eval_iters), train_iter):\n",
    "                    _, loss = model(x, y)\n",
    "                    train_losses.append(loss.item())\n",
    "                train_loss = np.mean(train_losses)\n",
    "\n",
    "                print(f\"step {iter}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "            model.train()\n",
    "\n",
    "        # Training logic\n",
    "        x, y = next(train_iter)  # replace get_batch\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Zone\n",
    "\n",
    "From nanoGPT:\n",
    "\n",
    "```python\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_normal(\n",
    "    batch_size: int,\n",
    "    seq_len: int,\n",
    "    mask_type: str = \"causal\",  # \"causal\", \"padding\", or \"bidirectional\"\n",
    "    device: Optional[torch.device] = None\n",
    ") -> Bool[Tensor, \"batch seq_len seq_len\"]:\n",
    "    \n",
    "    # x-transformer style (support multiple mask types)\n",
    "    def impl1():\n",
    "        if mask_type == \"causal\":\n",
    "            # Create lower triangular mask\n",
    "            mask = torch.ones((seq_len, seq_len), device=device).triu(1).bool()\n",
    "            mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        elif mask_type == \"padding\":\n",
    "            # Generate random padding mask for testing\n",
    "            mask = torch.rand(batch_size, seq_len) > 0.2\n",
    "            mask = mask.unsqueeze(1) & mask.unsqueeze(2)\n",
    "        else:  # bidirectional\n",
    "            mask = torch.ones((batch_size, seq_len, seq_len), device=device).bool()\n",
    "        return ~mask  # Note: here we take the inverse, so True means \"allow attend\"\n",
    "    \n",
    "    # lit-gpt style (focus on causal mask + cache optimization)\n",
    "    def impl2():\n",
    "        if mask_type != \"causal\":\n",
    "            raise NotImplementedError(\"lit-gpt mainly supports causal mask\")\n",
    "        # Precompute and cache mask\n",
    "        ones = torch.ones((seq_len, seq_len), dtype=torch.bool, device=device)\n",
    "        mask = torch.tril(ones).unsqueeze(0)\n",
    "        return mask.expand(batch_size, -1, -1)\n",
    "    \n",
    "    # nanoGPT style\n",
    "    def impl3():\n",
    "        if mask_type == \"causal\":\n",
    "            mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "            return mask.view(1, seq_len, seq_len).expand(batch_size, -1, -1)\n",
    "        return torch.ones((batch_size, seq_len, seq_len), device=device)\n",
    "    \n",
    "    mask1 = impl1()\n",
    "    mask2 = impl2()\n",
    "    mask3 = impl3()\n",
    "    \n",
    "    tprint('shape', sep='*')\n",
    "    cprint(mask1.shape)\n",
    "    assert mask1.shape == mask2.shape == mask3.shape\n",
    "    print('passed')\n",
    "    \n",
    "    if mask_type == \"causal\":\n",
    "        tprint('allclose', sep='*')\n",
    "        assert torch.allclose(mask1.float(), mask2.float())\n",
    "        assert torch.allclose(mask1.float(), mask3.float())\n",
    "        print('passed')\n",
    "    \n",
    "    return mask1\n",
    "\n",
    "\n",
    "# mask_normal(batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_post_normal(\n",
    "    attn_weights: Float[Tensor, \"batch num_heads seq_len seq_len\"],\n",
    "    dropout_p: float = 0.1\n",
    ") -> Float[Tensor, \"batch num_heads seq_len seq_len\"]:\n",
    "    batch, num_heads, seq_len, _ = attn_weights.shape\n",
    "    \n",
    "    # x-transformer style (support talking heads etc.)\n",
    "    def impl1(attn):\n",
    "        # Simulate talking heads\n",
    "        talking_heads = nn.Linear(num_heads, num_heads, bias=False)\n",
    "        talking_heads.weight.data = torch.eye(num_heads)  # Initialize as identity matrix\n",
    "        \n",
    "        attn = rearrange(attn, 'b h i j -> b i j h')\n",
    "        attn = talking_heads(attn)\n",
    "        attn = rearrange(attn, 'b i j h -> b h i j')\n",
    "        \n",
    "        attn = F.dropout(attn, p=dropout_p)\n",
    "        return attn\n",
    "    \n",
    "    # lit-gpt style\n",
    "    def impl2(attn):\n",
    "        return F.dropout(attn, p=dropout_p)\n",
    "    \n",
    "    # nanoGPT style\n",
    "    def impl3(attn):\n",
    "        return F.dropout(attn, p=dropout_p)\n",
    "    \n",
    "    out1 = impl1(attn_weights)\n",
    "    out2 = impl2(attn_weights)\n",
    "    out3 = impl3(attn_weights)\n",
    "    \n",
    "    tprint('shape', sep='*')\n",
    "    cprint(out1.shape)\n",
    "    assert out1.shape == out2.shape == out3.shape\n",
    "    print('passed')\n",
    "    \n",
    "    return out1\n",
    "\n",
    "\n",
    "# attention_post_normal(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kv_cache_normal(\n",
    "    k: Float[Tensor, \"batch num_heads seq_len head_dim\"],\n",
    "    v: Float[Tensor, \"batch num_heads seq_len head_dim\"],\n",
    "    past_key_values: Optional[Tuple[Tensor, Tensor]] = None,\n",
    "    use_cache: bool = True\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    batch, num_heads, seq_len, head_dim = k.shape\n",
    "    \n",
    "    # x-transformer style\n",
    "    def impl1():\n",
    "        if past_key_values is not None:\n",
    "            past_k, past_v = past_key_values\n",
    "            k = torch.cat([past_k, k], dim=2)\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        if use_cache:\n",
    "            return (k, v), (k, v)\n",
    "        return (k, v), None\n",
    "    \n",
    "    # lit-gpt style\n",
    "    def impl2():\n",
    "        if past_key_values is not None:\n",
    "            past_k, past_v = past_key_values\n",
    "            # Use efficient indexing to copy\n",
    "            k_cache = torch.empty_like(past_k)\n",
    "            v_cache = torch.empty_like(past_v)\n",
    "            k_cache[:, :, :-seq_len] = past_k[:, :, seq_len:]\n",
    "            v_cache[:, :, :-seq_len] = past_v[:, :, seq_len:]\n",
    "            k_cache[:, :, -seq_len:] = k\n",
    "            v_cache[:, :, -seq_len:] = v\n",
    "            k, v = k_cache, v_cache\n",
    "            \n",
    "        if use_cache:\n",
    "            return (k, v), (k, v)\n",
    "        return (k, v), None\n",
    "    \n",
    "    # nanoGPT style\n",
    "    def impl3():\n",
    "        if past_key_values is not None:\n",
    "            past_k, past_v = past_key_values\n",
    "            k = torch.cat([past_k, k], dim=2)\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        if use_cache:\n",
    "            return (k, v), (k, v)\n",
    "        return (k, v), None\n",
    "    \n",
    "    (k1, v1), cache1 = impl1()\n",
    "    (k2, v2), cache2 = impl2()\n",
    "    (k3, v3), cache3 = impl3()\n",
    "    \n",
    "    tprint('shape', sep='*')\n",
    "    cprint(k1.shape, v1.shape)\n",
    "    assert k1.shape == k2.shape == k3.shape\n",
    "    assert v1.shape == v2.shape == v3.shape\n",
    "    print('passed')\n",
    "   \n",
    "    tprint('allclose', sep='*')\n",
    "    assert torch.allclose(k1, k3, rtol=1e-4)\n",
    "    assert torch.allclose(v1, v3, rtol=1e-4)\n",
    "    if use_cache:\n",
    "        assert all(torch.allclose(c1, c3, rtol=1e-4) \n",
    "                  for c1, c3 in zip(cache1, cache3))\n",
    "    print('passed')\n",
    "    \n",
    "    return (k1, v1), cache1\n",
    "\n",
    "\n",
    "# kv_cache_normal(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModularAttention(nn.Module):\n",
    "    \"\"\"Configurable modular attention implementation\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        projection_type: str = \"unified\",\n",
    "        qkv_bias: bool = False,\n",
    "        attn_dropout: float = 0.0,\n",
    "        scaling_type: str = \"default\"  # or \"learned\" or \"fixed\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        \n",
    "        # QKV projection configuration\n",
    "        if projection_type == \"unified\":\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        elif projection_type == \"separated\":\n",
    "            self.to_q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.to_kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        else:  # INDIVIDUAL\n",
    "            self.to_q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.to_k = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.to_v = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            \n",
    "        # Scaling configuration\n",
    "        self.scaling_type = scaling_type\n",
    "        if scaling_type == \"learned\":\n",
    "            self.scale = nn.Parameter(torch.ones(1) / np.sqrt(self.head_dim))\n",
    "        else:\n",
    "            self.scale = 1.0 / np.sqrt(self.head_dim)\n",
    "            \n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        \n",
    "    def _project_qkv(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Implement QKV projection based on different projection_type\"\"\"\n",
    "        if hasattr(self, 'qkv'):\n",
    "            qkv = self.qkv(x)\n",
    "            q, k, v = qkv.chunk(3, dim=-1)\n",
    "        elif hasattr(self, 'to_kv'):\n",
    "            q = self.to_q(x)\n",
    "            k, v = self.to_kv(x).chunk(2, dim=-1)\n",
    "        else:\n",
    "            q = self.to_q(x)\n",
    "            k = self.to_k(x)\n",
    "            v = self.to_v(x)\n",
    "            \n",
    "        return map(\n",
    "            lambda t: t.view(t.shape[0], -1, self.num_heads, self.head_dim).transpose(1, 2),\n",
    "            (q, k, v)\n",
    "        )\n",
    "    \n",
    "    def _apply_attention(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Calculate attention scores and apply to values\"\"\"\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1))\n",
    "        \n",
    "        # Apply scaling\n",
    "        if isinstance(self.scale, nn.Parameter):\n",
    "            attn_weights = attn_weights * self.scale\n",
    "        else:\n",
    "            attn_weights = attn_weights * self.scale\n",
    "            \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
    "            \n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        return output, attn_weights\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Complete attention forward pass\"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # 1. QKV projection\n",
    "        q, k, v = self._project_qkv(x)\n",
    "        \n",
    "        # 2. Calculate attention\n",
    "        out, weights = self._apply_attention(q, k, v, mask)\n",
    "        \n",
    "        # 3. Reshape output\n",
    "        out = out.transpose(1, 2).contiguous().view(B, N, C)\n",
    "        \n",
    "        return out, weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
