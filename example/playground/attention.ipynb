{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref:\n",
    "- https://github.com/TimS-ml/nanoGPT\n",
    "- https://youtu.be/kCc8FmEb1nY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from boring_llm_base.constants import PROJECT_HOME_DIR\n",
    "import sys; sys.path.append(str(PROJECT_HOME_DIR)); os.chdir(PROJECT_HOME_DIR)\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from jaxtyping import Float, Bool\n",
    "\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "from boring_utils.utils import (\n",
    "    cprint, \n",
    "    tprint, \n",
    "    get_device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> device:\u001b[0m\n",
      "device(type='mps')\n"
     ]
    }
   ],
   "source": [
    "# from boring_nn.attention.config import AttentionConfig\n",
    "# cfg = AttentionConfig()\n",
    "# cprint(cfg)\n",
    "\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # time steps (seq length, context window)\n",
    "n_embed = 36    # channels (embedding dim)\n",
    "\n",
    "t_enc, t_dec = 10, block_size  # encoder/decoder sequence lengths \n",
    "n_head = 6\n",
    "assert n_embed % n_head == 0\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "max_iters = 100\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "\n",
    "device = get_device()\n",
    "# vocab_size = len(set(text))\n",
    "cprint(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('DATA_DIR', './data/')\n",
    "data_dir = os.path.join(data_dir, 'enwik8')\n",
    "\n",
    "# # NOTE: only read enwik8 first 10M bytes\n",
    "# with gzip.open(os.path.join(data_dir, 'enwik8.gz')) as file:\n",
    "#     text = file.read(int(10e6)).decode('utf-8')\n",
    "\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    stoi = meta['stoi']\n",
    "    itos = meta['itos']\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Meta file {meta_path} not found\")\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) \n",
    "\n",
    "train_bin_path = os.path.join(data_dir, 'train.bin')\n",
    "val_bin_path = os.path.join(data_dir, 'val.bin')\n",
    "\n",
    "# train_tensor = torch.tensor(encode(data), dtype=torch.long) # convert to tensor\n",
    "\n",
    "# torch.long is just an alias for torch.int64\n",
    "# load the binary data\n",
    "train_data = np.fromfile(train_bin_path, dtype=np.uint16)\n",
    "val_data = np.fromfile(val_bin_path, dtype=np.uint16)\n",
    "\n",
    "# convert to pytorch tensors\n",
    "train_data = torch.from_numpy(train_data.astype(np.int64))\n",
    "val_data = torch.from_numpy(val_data.astype(np.int64))\n",
    "\n",
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = int(block_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # single sample\n",
    "        ix = torch.randint(\n",
    "            len(self.data) - self.block_size - 1, (1,)\n",
    "        )\n",
    "        full_seq = self.data[ix:ix + self.block_size + 1]\n",
    "        x = full_seq[:-1]\n",
    "        y = full_seq[1:]\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.block_size\n",
    "\n",
    "\n",
    "train_dataset = TextSamplerDataset(train_data, block_size)\n",
    "val_dataset = TextSamplerDataset(val_data, block_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (embedding): Embedding(2102, 2102)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        embedding_dim = vocab_size\n",
    "        # embedding_dim = 128\n",
    "        # each token is represented by a one-hot vector\n",
    "        # directly reads off the logits for the next token from the embedding table\n",
    "        # for example: 24 will reads off the 24th column of the embedding table\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is (batch_size, block_size)\n",
    "        logits = self.embedding(idx)  # B, T, C: (batch_size, block_size, embedding_dim)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # (batch_size * block_size, embedding_dim)\n",
    "            targets = targets.view(-1)  # (batch_size * block_size)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "train_iter = cycle(train_loader)\n",
    "val_iter = cycle(val_loader)\n",
    "\n",
    "def train(\n",
    "        model: nn.Module = model,\n",
    "        train_iter: DataLoader = train_iter,\n",
    "        val_iter: DataLoader = val_iter,\n",
    "        eval_iters: int = eval_iters,\n",
    "        max_iters: int = max_iters,\n",
    "        eval_interval: int = eval_interval,\n",
    "    ):\n",
    "    for iter in range(max_iters):\n",
    "        # Eval logic\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_losses = []\n",
    "                for _, (x, y) in zip(range(eval_iters), val_iter):\n",
    "                    _, loss = model(x, y)\n",
    "                    val_losses.append(loss.item())\n",
    "                val_loss = np.mean(val_losses)\n",
    "\n",
    "                train_losses = []\n",
    "                for _, (x, y) in zip(range(eval_iters), train_iter):\n",
    "                    _, loss = model(x, y)\n",
    "                    train_losses.append(loss.item())\n",
    "                train_loss = np.mean(train_losses)\n",
    "\n",
    "                print(f\"step {iter}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "            model.train()\n",
    "\n",
    "        # Training logic\n",
    "        x, y = next(train_iter)  # replace get_batch\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Zone\n",
    "\n",
    "From nanoGPT:\n",
    "\n",
    "```python\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      "******************** qkv_normal -> allclose ********************\u001b[0m\n",
      "passed\n",
      "\u001b[35m\n",
      "******************** qkv_normal -> shape ********************\u001b[0m\n",
      "passed\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(batch_size, block_size, n_embed)\n",
    "\n",
    "def qkv_normal(\n",
    "    x: Float[Tensor, \"batch seq_len embedding_dim\"],\n",
    "    num_heads: int = n_head\n",
    "):\n",
    "    batch, seq_len, dim = x.shape\n",
    "    head_dim = dim // num_heads\n",
    "    \n",
    "    # Fixed weights, for testing `assert torch.allclose`\n",
    "    fixed_weight = torch.randn(dim, dim)\n",
    "\n",
    "    # Separated QKV (x-transformer style)\n",
    "    def impl1(x):\n",
    "        to_q = nn.Linear(dim, dim, bias=False)\n",
    "        to_kv = nn.Linear(dim, dim * 2, bias=False)\n",
    "        to_q.weight.data = fixed_weight.clone()\n",
    "        to_kv.weight.data = torch.cat([fixed_weight, fixed_weight]).clone()\n",
    "\n",
    "        q = to_q(x)\n",
    "        k, v = to_kv(x).chunk(2, dim=-1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'batch seq_len (num_heads head_dim) -> batch num_heads seq_len head_dim', num_heads = num_heads), (q, k, v))\n",
    "        return q, k, v\n",
    "\n",
    "    # Unified QKV (lit-gpt style)\n",
    "    def impl2(x):\n",
    "        to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        to_qkv.weight.data = torch.cat([fixed_weight] * 3).clone() \n",
    "\n",
    "        # Split into q,k,v and reshape to add head dimension\n",
    "        q, k, v  = to_qkv(x).split(n_embed, dim=2)\n",
    "        # Permute to get (batch, num_heads, seq_len, head_dim)\n",
    "        q, k, v = map(\n",
    "            lambda t: t.view(batch, -1, num_heads, head_dim).transpose(1, 2),\n",
    "            (q, k, v)\n",
    "        )\n",
    "\n",
    "        return q, k, v\n",
    "\n",
    "    # nanoGPT style\n",
    "    def impl3(x):\n",
    "        to_q = nn.Linear(dim, head_dim, bias=False)\n",
    "        to_k = nn.Linear(dim, head_dim, bias=False)\n",
    "        to_v = nn.Linear(dim, head_dim, bias=False)\n",
    "        to_q.weight.data = fixed_weight.clone()\n",
    "        to_k.weight.data = fixed_weight.clone()\n",
    "        to_v.weight.data = fixed_weight.clone()\n",
    "        \n",
    "        k = to_k(x)\n",
    "        q = to_q(x)\n",
    "        v = to_v(x)\n",
    "        \n",
    "        q = q.view(batch, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "        k = k.view(batch, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "        v = v.view(batch, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "        return q, k, v    \n",
    "\n",
    "    # Compare results\n",
    "    q1, k1, v1 = impl1(x)\n",
    "    q2, k2, v2 = impl2(x)\n",
    "    q3, k3, v3 = impl3(x)\n",
    "    \n",
    "    tprint('allclose', sep='*')\n",
    "    assert torch.allclose(q1, q2, rtol=1e-4)\n",
    "    assert torch.allclose(k1, k2, rtol=1e-4)\n",
    "    assert torch.allclose(v1, v2, rtol=1e-4)\n",
    "    assert torch.allclose(q1, q3, rtol=1e-4)\n",
    "    assert torch.allclose(k1, k3, rtol=1e-4)\n",
    "    assert torch.allclose(v1, v3, rtol=1e-4)\n",
    "    print('passed')\n",
    "\n",
    "    tprint('shape', sep='*')\n",
    "    try:\n",
    "        assert q1.shape == q2.shape\n",
    "        assert k1.shape == k2.shape\n",
    "        assert v1.shape == v2.shape\n",
    "        assert q1.shape == q3.shape\n",
    "        assert k1.shape == k3.shape\n",
    "        assert v1.shape == v3.shape\n",
    "        print('passed')\n",
    "    except Exception as e:\n",
    "        cprint(q1.shape, k1.shape, v1.shape)\n",
    "        cprint(q2.shape, k2.shape, v2.shape)\n",
    "        cprint(q3.shape, k3.shape, v3.shape)\n",
    "        raise e\n",
    "\n",
    "\n",
    "qkv_normal(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped QKV\n",
    "\n",
    "From lit-gpt:\n",
    "```\n",
    "to use multi-head attention (MHA), set this to `n_head` (default)\n",
    "to use multi-query attention (MQA), set this to 1\n",
    "to use grouped-query attention (GQA), set this to a value in between\n",
    "Example with `n_head=4`\n",
    "┌───┐┌───┐┌───┐┌───┐     ┌───┐    ┌───┐             ┌───┐\n",
    "│ v ││ v ││ v ││ v │     │ v │    │ v │             │ v │\n",
    "└───┘└───┘└───┘└───┘     └───┘    └───┘             └───┘\n",
    "  │    │    │    │         │        │                 │\n",
    "┌───┐┌───┐┌───┐┌───┐     ┌───┐    ┌───┐             ┌───┐\n",
    "│ k ││ k ││ k ││ k │     │ k │    │ k │             │ k │\n",
    "└───┘└───┘└───┘└───┘     └───┘    └───┘             └───┘\n",
    "  │    │    │    │      ┌──┴──┐  ┌──┴──┐      ┌────┬──┴─┬────┐\n",
    "┌───┐┌───┐┌───┐┌───┐  ┌───┐┌───┐┌───┐┌───┐  ┌───┐┌───┐┌───┐┌───┐\n",
    "│ q ││ q ││ q ││ q │  │ q ││ q ││ q ││ q │  │ q ││ q ││ q ││ q │\n",
    "└───┘└───┘└───┘└───┘  └───┘└───┘└───┘└───┘  └───┘└───┘└───┘└───┘\n",
    "◀──────────────────▶  ◀──────────────────▶  ◀──────────────────▶\n",
    "        MHA                    GQA                   MQA\n",
    "  n_query_groups=4       n_query_groups=2      n_query_groups=1\n",
    "\n",
    "credit https://arxiv.org/pdf/2305.13245.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      "==================== <module> -> MHA ====================\u001b[0m\n",
      "\u001b[35m\n",
      "******************** qkv_grouped -> shape ********************\u001b[0m\n",
      "passed\n",
      "\u001b[35m\n",
      "==================== <module> -> MQA ====================\u001b[0m\n",
      "\u001b[35m\n",
      "******************** qkv_grouped -> shape ********************\u001b[0m\n",
      "passed\n",
      "\u001b[35m\n",
      "==================== <module> -> GQA ====================\u001b[0m\n",
      "\u001b[35m\n",
      "******************** qkv_grouped -> shape ********************\u001b[0m\n",
      "passed\n"
     ]
    }
   ],
   "source": [
    "def qkv_grouped(\n",
    "    x: Float[Tensor, \"batch seq_len embedding_dim\"],\n",
    "    num_heads: int = n_head,\n",
    "    n_query_groups: int = 2\n",
    "):\n",
    "    batch, seq_len, dim = x.shape\n",
    "    head_dim = dim // num_heads\n",
    "    \n",
    "    # TODO: Fixed weights for testing\n",
    "    # q_weight = torch.randn(dim, dim)\n",
    "    # k_weight = torch.randn(dim, head_dim * n_query_groups)\n",
    "    # v_weight = torch.randn(dim, head_dim * n_query_groups)\n",
    "\n",
    "    # 1. x-transformer style implementation (Separated QKV + GQA)\n",
    "    def impl1(x):\n",
    "        kv_heads = n_query_groups\n",
    "        to_q = nn.Linear(dim, dim, bias=False)\n",
    "        to_kv = nn.Linear(dim, head_dim * kv_heads * 2, bias=False)\n",
    "        # to_q.weight.data = q_weight.clone()\n",
    "        # to_kv.weight.data = torch.cat([k_weight, v_weight], dim=1)\n",
    "\n",
    "        # tprint(f'grouped heads={kv_heads}', sep='-')\n",
    "        # cprint(dim, head_dim)\n",
    "        # cprint(to_q.weight.shape, to_kv.weight.shape)\n",
    "\n",
    "        q = to_q(x)\n",
    "        k, v = to_kv(x).chunk(2, dim=-1)\n",
    "\n",
    "        # Handle grouped-query attention\n",
    "        if kv_heads == 1:  # MQA case\n",
    "            # method 1\n",
    "            # k, v = tuple(rearrange(t, 'b n (h d) -> b h n d', h=kv_heads) for t in (k, v))\n",
    "            # k, v = tuple(repeat(t, 'b h n d -> b (r h) n d', r=num_heads // kv_heads) for t in (k, v))\n",
    "\n",
    "            # method 2\n",
    "            # k, v = map(\n",
    "            #     lambda t: repeat(\n",
    "            #         rearrange(t, 'b n (h d) -> b h n d', h=kv_heads),\n",
    "            #         'b h n d -> b (r h) n d',\n",
    "            #         r=num_heads // kv_heads\n",
    "            #     ),\n",
    "            #     (k, v)\n",
    "            # )\n",
    "\n",
    "            # method 3\n",
    "            k, v = tuple(repeat(t, 'b n (h d) -> b (r h) n d', h=kv_heads, r=num_heads // kv_heads) for t in (k, v))\n",
    "        elif kv_heads < num_heads:  # GQA case\n",
    "            k, v = tuple(repeat(t, 'b n (h d) -> b (r h) n d', h=kv_heads, r=num_heads // kv_heads) for t in (k, v))\n",
    "        else:  # MHA case\n",
    "            k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=num_heads), (k, v)) \n",
    "\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h=num_heads)\n",
    "        return q, k, v\n",
    "\n",
    "    # 2. lit-gpt style implementation (Unified QKV + GQA)\n",
    "    def impl2(x):\n",
    "        # Calculate number of Q per KV group\n",
    "        q_per_kv = num_heads // n_query_groups\n",
    "        total_qkv = q_per_kv + 2  # Each group has q_per_kv queries + 1 key + 1 value\n",
    "        qkv_dim = (num_heads + 2 * n_query_groups) * head_dim\n",
    "\n",
    "        to_qkv = nn.Linear(dim, qkv_dim, bias=False)\n",
    "        # qkv_weight = torch.cat([\n",
    "        #     q_weight,\n",
    "        #     k_weight.repeat(q_per_kv, 1),\n",
    "        #     v_weight.repeat(q_per_kv, 1)\n",
    "        # ], dim=1)\n",
    "        # to_qkv.weight.data = qkv_weight.clone()\n",
    "\n",
    "        qkv = to_qkv(x)\n",
    "        qkv = qkv.view(batch, seq_len, n_query_groups, total_qkv, head_dim)\n",
    "        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)\n",
    "        \n",
    "        # Split Q, K, V\n",
    "        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)\n",
    "        \n",
    "        # Handle K, V expansion for MQA/GQA\n",
    "        if n_query_groups != num_heads:\n",
    "            k = k.expand(batch, n_query_groups, q_per_kv, seq_len, head_dim)\n",
    "            v = v.expand(batch, n_query_groups, q_per_kv, seq_len, head_dim)\n",
    "        \n",
    "        # Reshape to final shape\n",
    "        q = q.reshape(batch, -1, seq_len, head_dim)  # (B, nh_q, T, hs)\n",
    "        k = k.reshape(batch, -1, seq_len, head_dim)  # (B, nh_k, T, hs)\n",
    "        v = v.reshape(batch, -1, seq_len, head_dim)  # (B, nh_v, T, hs)\n",
    "        \n",
    "        return q, k, v\n",
    "\n",
    "    # Validate implementations\n",
    "    q1, k1, v1 = impl1(x)\n",
    "    q2, k2, v2 = impl2(x)\n",
    "    \n",
    "    # tprint('allclose', sep='*')\n",
    "    # assert torch.allclose(q1, q2, rtol=1e-4)\n",
    "    # assert torch.allclose(k1, k2, rtol=1e-4)\n",
    "    # assert torch.allclose(v1, v2, rtol=1e-4)\n",
    "    # print('passed')\n",
    "\n",
    "    tprint('shape', sep='*')\n",
    "    try:\n",
    "        assert q1.shape == q2.shape\n",
    "        assert k1.shape == k2.shape\n",
    "        assert v1.shape == v2.shape\n",
    "        print('passed')\n",
    "    except Exception as e:\n",
    "        cprint(q1.shape, k1.shape, v1.shape)\n",
    "        cprint(q2.shape, k2.shape, v2.shape)\n",
    "        raise e\n",
    "\n",
    "\n",
    "# Test different GQA configurations\n",
    "x = torch.randn(batch_size, block_size, n_embed)\n",
    "\n",
    "# Test standard multi-head attention\n",
    "tprint('MHA')\n",
    "qkv_grouped(x, num_heads=n_head, n_query_groups=n_head)\n",
    "\n",
    "# Test MQA (1 KV head)\n",
    "tprint('MQA')\n",
    "qkv_grouped(x, num_heads=n_head, n_query_groups=1)\n",
    "\n",
    "# Test GQA (e.g. 8 heads, 2 groups)\n",
    "tprint('GQA')\n",
    "qkv_grouped(x, num_heads=n_head, n_query_groups=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_normal(\n",
    "    x: Float[Tensor, \"batch seq_len embedding_dim\"],\n",
    "    num_heads: int = n_head\n",
    "):\n",
    "    batch, seq_len, dim = x.shape\n",
    "    head_dim = dim // num_heads\n",
    "    \n",
    "    # Fixed weights for testing\n",
    "    fixed_weight = torch.randn(seq_len, dim)\n",
    "\n",
    "    # x-transformer style (using sin-cos)\n",
    "    def impl1(x):\n",
    "        pe = fixed_weight.clone()\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        return x + pe\n",
    "\n",
    "    # lit-gpt style (using RoPE)\n",
    "    def impl2(x):\n",
    "        # Generate sin/cos for RoPE\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2) / head_dim))\n",
    "        t = torch.arange(seq_len)\n",
    "        freqs = torch.einsum('i,j->ij', t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos().view(1, seq_len, 1, head_dim)\n",
    "        sin = emb.sin().view(1, seq_len, 1, head_dim)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        x_reshaped = x.view(batch, seq_len, num_heads, head_dim)\n",
    "        x_rope = apply_rope(x_reshaped, cos, sin)\n",
    "        return x_rope.view(batch, seq_len, -1)\n",
    "\n",
    "    # nanoGPT style (simple positional embedding)\n",
    "    def impl3(x):\n",
    "        position_embeddings = nn.Embedding(seq_len, dim)\n",
    "        position_embeddings.weight.data = fixed_weight.clone()\n",
    "        \n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        position_embeds = position_embeddings(positions)\n",
    "        return x + position_embeds\n",
    "\n",
    "    # Compare results\n",
    "    out1 = impl1(x)\n",
    "    out2 = impl2(x)\n",
    "    out3 = impl3(x)\n",
    "    \n",
    "    tprint('shape', sep='*')\n",
    "    cprint(out1.shape, out2.shape, out3.shape)\n",
    "    assert out1.shape == out2.shape == out3.shape\n",
    "    print('passed')\n",
    "\n",
    "    return out1\n",
    "\n",
    "\n",
    "# positional_encoding_normal(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_normal(\n",
    "    q: Float[Tensor, \"batch num_heads seq_len head_dim\"],\n",
    "    k: Float[Tensor, \"batch num_heads seq_len head_dim\"],\n",
    "    v: Float[Tensor, \"batch num_heads seq_len head_dim\"],\n",
    "    mask: Optional[Bool[Tensor, \"batch seq_len seq_len\"]] = None\n",
    "):\n",
    "    batch, num_heads, seq_len, head_dim = q.shape\n",
    "    scale = 1.0 / math.sqrt(head_dim)\n",
    "\n",
    "    # x-transformer style\n",
    "    def impl1(q, k, v):\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * scale\n",
    "        \n",
    "        if exists(mask):\n",
    "            sim = sim.masked_fill(~mask, float('-inf'))\n",
    "            \n",
    "        attn = F.softmax(sim, dim=-1)\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        return out, attn\n",
    "\n",
    "    # lit-gpt style (use torch.nn.functional)\n",
    "    def impl2(q, k, v):\n",
    "        if hasattr(F, 'scaled_dot_product_attention'):\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q.transpose(1, 2),  # (b, seq, head, dim)\n",
    "                k.transpose(1, 2),\n",
    "                v.transpose(1, 2),\n",
    "                attn_mask=mask,\n",
    "                dropout_p=0.0,\n",
    "                is_causal=mask is None\n",
    "            )\n",
    "            return out.transpose(1, 2), None  # 返回None作为attn weights\n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        if exists(mask):\n",
    "            attn = attn.masked_fill(~mask, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        return out, attn\n",
    "\n",
    "    # nanoGPT style\n",
    "    def impl3(q, k, v):\n",
    "        att = (q @ k.transpose(-2, -1)) * scale\n",
    "        if exists(mask):\n",
    "            att = att.masked_fill(~mask, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        out = att @ v\n",
    "        return out, att\n",
    "\n",
    "    out1, attn1 = impl1(q, k, v)\n",
    "    out2, attn2 = impl2(q, k, v)\n",
    "    out3, attn3 = impl3(q, k, v)\n",
    "    \n",
    "    tprint('shape', sep='*')\n",
    "    cprint(out1.shape)\n",
    "    assert out1.shape == out2.shape == out3.shape\n",
    "    if attn2 is not None:  # if not using SDPA\n",
    "        assert attn1.shape == attn2.shape == attn3.shape\n",
    "    print('passed')\n",
    "    \n",
    "    tprint('allclose', sep='*')\n",
    "    assert torch.allclose(out1, out3, rtol=1e-4)\n",
    "    if attn2 is not None:  # if not using SDPA\n",
    "        assert torch.allclose(out1, out2, rtol=1e-4)\n",
    "        assert torch.allclose(attn1, attn3, rtol=1e-4)\n",
    "    print('passed')\n",
    "\n",
    "    return out1, attn1\n",
    "\n",
    "\n",
    "# attention_normal(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_normal(\n",
    "    batch_size: int,\n",
    "    seq_len: int,\n",
    "    mask_type: str = \"causal\",  # \"causal\", \"padding\", or \"bidirectional\"\n",
    "    device: Optional[torch.device] = None\n",
    ") -> Bool[Tensor, \"batch seq_len seq_len\"]:\n",
    "    \n",
    "    # x-transformer style (support multiple mask types)\n",
    "    def impl1():\n",
    "        if mask_type == \"causal\":\n",
    "            # Create lower triangular mask\n",
    "            mask = torch.ones((seq_len, seq_len), device=device).triu(1).bool()\n",
    "            mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        elif mask_type == \"padding\":\n",
    "            # Generate random padding mask for testing\n",
    "            mask = torch.rand(batch_size, seq_len) > 0.2\n",
    "            mask = mask.unsqueeze(1) & mask.unsqueeze(2)\n",
    "        else:  # bidirectional\n",
    "            mask = torch.ones((batch_size, seq_len, seq_len), device=device).bool()\n",
    "        return ~mask  # Note: here we take the inverse, so True means \"allow attend\"\n",
    "    \n",
    "    # lit-gpt style (focus on causal mask + cache optimization)\n",
    "    def impl2():\n",
    "        if mask_type != \"causal\":\n",
    "            raise NotImplementedError(\"lit-gpt mainly supports causal mask\")\n",
    "        # Precompute and cache mask\n",
    "        ones = torch.ones((seq_len, seq_len), dtype=torch.bool, device=device)\n",
    "        mask = torch.tril(ones).unsqueeze(0)\n",
    "        return mask.expand(batch_size, -1, -1)\n",
    "    \n",
    "    # nanoGPT style\n",
    "    def impl3():\n",
    "        if mask_type == \"causal\":\n",
    "            mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "            return mask.view(1, seq_len, seq_len).expand(batch_size, -1, -1)\n",
    "        return torch.ones((batch_size, seq_len, seq_len), device=device)\n",
    "    \n",
    "    mask1 = impl1()\n",
    "    mask2 = impl2()\n",
    "    mask3 = impl3()\n",
    "    \n",
    "    tprint('shape', sep='*')\n",
    "    cprint(mask1.shape)\n",
    "    assert mask1.shape == mask2.shape == mask3.shape\n",
    "    print('passed')\n",
    "    \n",
    "    if mask_type == \"causal\":\n",
    "        tprint('allclose', sep='*')\n",
    "        assert torch.allclose(mask1.float(), mask2.float())\n",
    "        assert torch.allclose(mask1.float(), mask3.float())\n",
    "        print('passed')\n",
    "    \n",
    "    return mask1\n",
    "\n",
    "\n",
    "# mask_normal(batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_post_normal(\n",
    "    attn_weights: Float[Tensor, \"batch num_heads seq_len seq_len\"],\n",
    "    dropout_p: float = 0.1\n",
    ") -> Float[Tensor, \"batch num_heads seq_len seq_len\"]:\n",
    "    batch, num_heads, seq_len, _ = attn_weights.shape\n",
    "    \n",
    "    # x-transformer style (support talking heads etc.)\n",
    "    def impl1(attn):\n",
    "        # Simulate talking heads\n",
    "        talking_heads = nn.Linear(num_heads, num_heads, bias=False)\n",
    "        talking_heads.weight.data = torch.eye(num_heads)  # Initialize as identity matrix\n",
    "        \n",
    "        attn = rearrange(attn, 'b h i j -> b i j h')\n",
    "        attn = talking_heads(attn)\n",
    "        attn = rearrange(attn, 'b i j h -> b h i j')\n",
    "        \n",
    "        attn = F.dropout(attn, p=dropout_p)\n",
    "        return attn\n",
    "    \n",
    "    # lit-gpt style\n",
    "    def impl2(attn):\n",
    "        return F.dropout(attn, p=dropout_p)\n",
    "    \n",
    "    # nanoGPT style\n",
    "    def impl3(attn):\n",
    "        return F.dropout(attn, p=dropout_p)\n",
    "    \n",
    "    out1 = impl1(attn_weights)\n",
    "    out2 = impl2(attn_weights)\n",
    "    out3 = impl3(attn_weights)\n",
    "    \n",
    "    tprint('shape', sep='*')\n",
    "    cprint(out1.shape)\n",
    "    assert out1.shape == out2.shape == out3.shape\n",
    "    print('passed')\n",
    "    \n",
    "    return out1\n",
    "\n",
    "\n",
    "# attention_post_normal(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kv_cache_normal(\n",
    "    k: Float[Tensor, \"batch num_heads seq_len head_dim\"],\n",
    "    v: Float[Tensor, \"batch num_heads seq_len head_dim\"],\n",
    "    past_key_values: Optional[Tuple[Tensor, Tensor]] = None,\n",
    "    use_cache: bool = True\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    batch, num_heads, seq_len, head_dim = k.shape\n",
    "    \n",
    "    # x-transformer style\n",
    "    def impl1():\n",
    "        if past_key_values is not None:\n",
    "            past_k, past_v = past_key_values\n",
    "            k = torch.cat([past_k, k], dim=2)\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        if use_cache:\n",
    "            return (k, v), (k, v)\n",
    "        return (k, v), None\n",
    "    \n",
    "    # lit-gpt style\n",
    "    def impl2():\n",
    "        if past_key_values is not None:\n",
    "            past_k, past_v = past_key_values\n",
    "            # Use efficient indexing to copy\n",
    "            k_cache = torch.empty_like(past_k)\n",
    "            v_cache = torch.empty_like(past_v)\n",
    "            k_cache[:, :, :-seq_len] = past_k[:, :, seq_len:]\n",
    "            v_cache[:, :, :-seq_len] = past_v[:, :, seq_len:]\n",
    "            k_cache[:, :, -seq_len:] = k\n",
    "            v_cache[:, :, -seq_len:] = v\n",
    "            k, v = k_cache, v_cache\n",
    "            \n",
    "        if use_cache:\n",
    "            return (k, v), (k, v)\n",
    "        return (k, v), None\n",
    "    \n",
    "    # nanoGPT style\n",
    "    def impl3():\n",
    "        if past_key_values is not None:\n",
    "            past_k, past_v = past_key_values\n",
    "            k = torch.cat([past_k, k], dim=2)\n",
    "            v = torch.cat([past_v, v], dim=2)\n",
    "        \n",
    "        if use_cache:\n",
    "            return (k, v), (k, v)\n",
    "        return (k, v), None\n",
    "    \n",
    "    (k1, v1), cache1 = impl1()\n",
    "    (k2, v2), cache2 = impl2()\n",
    "    (k3, v3), cache3 = impl3()\n",
    "    \n",
    "    tprint('shape', sep='*')\n",
    "    cprint(k1.shape, v1.shape)\n",
    "    assert k1.shape == k2.shape == k3.shape\n",
    "    assert v1.shape == v2.shape == v3.shape\n",
    "    print('passed')\n",
    "   \n",
    "    tprint('allclose', sep='*')\n",
    "    assert torch.allclose(k1, k3, rtol=1e-4)\n",
    "    assert torch.allclose(v1, v3, rtol=1e-4)\n",
    "    if use_cache:\n",
    "        assert all(torch.allclose(c1, c3, rtol=1e-4) \n",
    "                  for c1, c3 in zip(cache1, cache3))\n",
    "    print('passed')\n",
    "    \n",
    "    return (k1, v1), cache1\n",
    "\n",
    "\n",
    "# kv_cache_normal(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModularAttention(nn.Module):\n",
    "    \"\"\"Configurable modular attention implementation\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        projection_type: str = \"unified\",\n",
    "        qkv_bias: bool = False,\n",
    "        attn_dropout: float = 0.0,\n",
    "        scaling_type: str = \"default\"  # or \"learned\" or \"fixed\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        \n",
    "        # QKV projection configuration\n",
    "        if projection_type == \"unified\":\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        elif projection_type == \"separated\":\n",
    "            self.to_q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.to_kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        else:  # INDIVIDUAL\n",
    "            self.to_q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.to_k = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.to_v = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            \n",
    "        # Scaling configuration\n",
    "        self.scaling_type = scaling_type\n",
    "        if scaling_type == \"learned\":\n",
    "            self.scale = nn.Parameter(torch.ones(1) / np.sqrt(self.head_dim))\n",
    "        else:\n",
    "            self.scale = 1.0 / np.sqrt(self.head_dim)\n",
    "            \n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        \n",
    "    def _project_qkv(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Implement QKV projection based on different projection_type\"\"\"\n",
    "        if hasattr(self, 'qkv'):\n",
    "            qkv = self.qkv(x)\n",
    "            q, k, v = qkv.chunk(3, dim=-1)\n",
    "        elif hasattr(self, 'to_kv'):\n",
    "            q = self.to_q(x)\n",
    "            k, v = self.to_kv(x).chunk(2, dim=-1)\n",
    "        else:\n",
    "            q = self.to_q(x)\n",
    "            k = self.to_k(x)\n",
    "            v = self.to_v(x)\n",
    "            \n",
    "        return map(\n",
    "            lambda t: t.view(t.shape[0], -1, self.num_heads, self.head_dim).transpose(1, 2),\n",
    "            (q, k, v)\n",
    "        )\n",
    "    \n",
    "    def _apply_attention(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Calculate attention scores and apply to values\"\"\"\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1))\n",
    "        \n",
    "        # Apply scaling\n",
    "        if isinstance(self.scale, nn.Parameter):\n",
    "            attn_weights = attn_weights * self.scale\n",
    "        else:\n",
    "            attn_weights = attn_weights * self.scale\n",
    "            \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
    "            \n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        return output, attn_weights\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Complete attention forward pass\"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # 1. QKV projection\n",
    "        q, k, v = self._project_qkv(x)\n",
    "        \n",
    "        # 2. Calculate attention\n",
    "        out, weights = self._apply_attention(q, k, v, mask)\n",
    "        \n",
    "        # 3. Reshape output\n",
    "        out = out.transpose(1, 2).contiguous().view(B, N, C)\n",
    "        \n",
    "        return out, weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
