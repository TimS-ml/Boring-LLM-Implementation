{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref:\n",
    "- https://github.com/TimS-ml/nanoGPT/blob/master/gpt.py\n",
    "- https://github.com/lucidrains/x-transformers/blob/main/examples/enwik8_simple/train.py\n",
    "\n",
    "wip\n",
    "You can specify the data directory by setting the `DATA_DIR` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from boring_llm_base.constants import PROJECT_HOME_DIR\n",
    "import sys; sys.path.append(str(PROJECT_HOME_DIR)); os.chdir(PROJECT_HOME_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boring_utils.utils import (\n",
    "    cprint, \n",
    "    tprint, \n",
    "    get_device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> device:\u001b[0m\n",
      "device(type='mps')\n"
     ]
    }
   ],
   "source": [
    "DEV = True \n",
    "\n",
    "if not DEV:\n",
    "    batch_size = 64  # how many independent sequences will we process in parallel?\n",
    "    block_size = 256  # what is the maximum context length for predictions?\n",
    "    # max_iters = 5000\n",
    "    max_iters = 4000\n",
    "    eval_interval = 500\n",
    "    learning_rate = 3e-4\n",
    "    eval_iters = 200\n",
    "    n_embd = 384\n",
    "    n_embed = n_embd\n",
    "    n_head = 6\n",
    "    n_layer = 6\n",
    "    dropout = 0.2\n",
    "\n",
    "else:\n",
    "    batch_size = 32\n",
    "    block_size = 8\n",
    "    # max_iters = 1000\n",
    "    max_iters = 100\n",
    "    eval_interval = 500\n",
    "    learning_rate = 3e-4\n",
    "    eval_iters = 200\n",
    "    n_embd = 32\n",
    "    n_embed = n_embd\n",
    "    n_head = 4\n",
    "    n_layer = 4\n",
    "    dropout = 0.2\n",
    "\n",
    "device = get_device()\n",
    "# vocab_size = len(set(text))\n",
    "cprint(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('DATA_DIR', './data')\n",
    "\n",
    "# read nanoGPT's shakespeare_char\n",
    "# with open(os.path.join(data_dir, 'shakespeare_char/input.txt'), 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# read enwik8 first 10M bytes\n",
    "with gzip.open(os.path.join(data_dir, 'enwik8/enwik8.gz')) as file:\n",
    "    text = file.read(int(10e6)).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char Level Tokenization\n",
    "\n",
    "[1] nanoGPT: create mapping from characters to indices\n",
    "\n",
    "```python\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# encode\n",
    "text = \"Hello\"\n",
    "tokens = [stoi[c] for c in text]  # char ids\n",
    "# decode\n",
    "decoded = ''.join([itos[t] for t in tokens])  # \"Hello\"\n",
    "```\n",
    "\n",
    "[2] x-transformers: ASCII, and the printable characters starting at 32\n",
    "\n",
    "```python\n",
    "# encode\n",
    "text = \"Hello\"\n",
    "tokens = [ord(c) for c in text]  # ASCII values\n",
    "\n",
    "# decode\n",
    "decoded = ''.join(chr(t) for t in tokens)  # \"Hello\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(set(text))\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preview the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> text[:1000]:\u001b[0m\n",
      "('<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" '\n",
      " 'xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" '\n",
      " 'xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ '\n",
      " 'http://www.mediawiki.org/xml/export-0.3.xsd\" version=\"0.3\" xml:lang=\"en\">\\n'\n",
      " '  <siteinfo>\\n'\n",
      " '    <sitename>Wikipedia</sitename>\\n'\n",
      " '    <base>http://en.wikipedia.org/wiki/Main_Page</base>\\n'\n",
      " '    <generator>MediaWiki 1.6alpha</generator>\\n'\n",
      " '    <case>first-letter</case>\\n'\n",
      " '      <namespaces>\\n'\n",
      " '      <namespace key=\"-2\">Media</namespace>\\n'\n",
      " '      <namespace key=\"-1\">Special</namespace>\\n'\n",
      " '      <namespace key=\"0\" />\\n'\n",
      " '      <namespace key=\"1\">Talk</namespace>\\n'\n",
      " '      <namespace key=\"2\">User</namespace>\\n'\n",
      " '      <namespace key=\"3\">User talk</namespace>\\n'\n",
      " '      <namespace key=\"4\">Wikipedia</namespace>\\n'\n",
      " '      <namespace key=\"5\">Wikipedia talk</namespace>\\n'\n",
      " '      <namespace key=\"6\">Image</namespace>\\n'\n",
      " '      <namespace key=\"7\">Image talk</namespace>\\n'\n",
      " '      <namespace key=\"8\">MediaWiki</namespace>\\n'\n",
      " '      <namespace key=\"9\">MediaWiki talk</namespace>\\n'\n",
      " '      <namespa')\n",
      "\u001b[93m<module> -> text[10000:11000]:\u001b[0m\n",
      "('ader in the [[American Indian Movement]], has repeatedly stated that he is '\n",
      " '&quot;an anarchist, and so are all [his] ancestors.&quot;\\n'\n",
      " '\\n'\n",
      " 'In 1793, in the thick of the [[French Revolution]], [[William Godwin]] '\n",
      " \"published ''An Enquiry Concerning Political Justice'' \"\n",
      " '[http://web.bilkent.edu.tr/Online/www.english.upenn.edu/jlynch/Frank/Godwin/pjtp.html]. '\n",
      " \"Although Godwin did not use the word ''anarchism'', many later anarchists \"\n",
      " 'have regarded this book as the first major anarchist text, and Godwin as the '\n",
      " '&quot;founder of philosophical anarchism.&quot; But at this point no '\n",
      " \"anarchist movement yet existed, and the term ''anarchiste'' was known mainly \"\n",
      " 'as an insult hurled by the [[bourgeois]] [[Girondins]] at more radical '\n",
      " 'elements in the [[French revolution]].\\n'\n",
      " '\\n'\n",
      " '==The first self-labelled anarchist==\\n'\n",
      " '[[Image:Pierre_Joseph_Proudhon.jpg|110px|thumb|left|Pierre Joseph '\n",
      " 'Proudhon]]\\n'\n",
      " '{{main articles|[[Pierre-Joseph Proudhon]] and [[Mutualism (economic '\n",
      " 'theory)]]}}\\n'\n",
      " '\\n'\n",
      " \"It is commonly held that it wasn't until [[Pierr\")\n",
      "\u001b[93m<module> -> text[-1000:]:\u001b[0m\n",
      "('|-\\n'\n",
      " '| [[Nathaniel Bliss]] || [[1762]] &amp;ndash; [[1764]]\\n'\n",
      " '|-\\n'\n",
      " \"| Rev'd [[Nevil Maskelyne]] || [[1765]] &amp;ndash; [[1811]]\\n\"\n",
      " '|-\\n'\n",
      " '| [[John Pond]] || [[1811]] &amp;ndash; [[1835]]\\n'\n",
      " '|-\\n'\n",
      " '| Sir [[George Airy | George Biddell Airy]] || [[1835]] &amp;ndash; '\n",
      " '[[1881]]\\n'\n",
      " '|-\\n'\n",
      " '| Sir [[William Christie (astronomer)|William Christie]] || [[1881]] '\n",
      " '&amp;ndash; [[1910]]\\n'\n",
      " '|-\\n'\n",
      " '| Sir [[Frank Dyson]] || [[1910]] &amp;ndash; [[1933]]\\n'\n",
      " '|-\\n'\n",
      " '| Sir [[Harold Spencer Jones]] || [[1933]] &amp;ndash; [[1955]]\\n'\n",
      " '|-\\n'\n",
      " '| Professor Sir [[Richard van der Riet Woolley]] || [[1956]] &amp;ndash; '\n",
      " '[[1971]]\\n'\n",
      " '|-\\n'\n",
      " '| Professor Sir [[Martin Ryle]] || [[1972]] &amp;ndash; [[1982]]\\n'\n",
      " '|-\\n'\n",
      " '| Professor Sir [[Francis Smith (astronomer)|Francis Graham-Smith]] || '\n",
      " '[[1982]] &amp;ndash; [[1990]]\\n'\n",
      " '|-\\n'\n",
      " '| Professor Sir [[Arnold Wolfendale]] || [[1991]] &amp;ndash; [[1995]]\\n'\n",
      " '|-\\n'\n",
      " '| [[Martin Rees]], Baron Rees of Ludlow || [[1995]] &amp;ndash; \\n'\n",
      " '|}\\n'\n",
      " '\\n'\n",
      " '[[Category:Astronomers]]\\n'\n",
      " '[[Category:Lists of British people]]\\n'\n",
      " '[[Category:Positions within the British Royal Household]]\\n'\n",
      " '\\n'\n",
      " '[[fr')\n"
     ]
    }
   ],
   "source": [
    "cprint(text[:1000])\n",
    "cprint(text[10000:11000])\n",
    "cprint(text[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> type(train_data):\u001b[0m\n",
      "<class 'torch.Tensor'>\n",
      "\u001b[93m<module> -> train_data.shape:\u001b[0m\n",
      "torch.Size([8956860])\n",
      "\u001b[93m<module> -> train_data[:10]:\u001b[0m\n",
      "tensor([30, 79, 71, 70, 75, 67, 89, 75, 77, 75])\n"
     ]
    }
   ],
   "source": [
    "cprint(type(train_data))\n",
    "cprint(train_data.shape)\n",
    "cprint(train_data[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "\n",
    "[1] Karpathy's get_batch numpy based data loader\n",
    "```python\n",
    "# randomly select len=batch_size start position\n",
    "# torch.randint(end, size)\n",
    "ix = torch.randint(\n",
    "    len(data) - block_size, \n",
    "    (batch_size,)\n",
    ")\n",
    "\n",
    "# then stack the data\n",
    "x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "# usage\n",
    "x, y = get_batch(split)\n",
    "logits, loss = model(x, y)\n",
    "```\n",
    "\n",
    "\n",
    "[2] Phil Wang's TextSamplerDataset torch based data loader\n",
    "```python\n",
    "# randomly select len=1 start position\n",
    "# torch.randint(0, end, size)\n",
    "ix = torch.randint(\n",
    "    0, \n",
    "    len(self.data) - self.block_size - 1, \n",
    "    (1,)\n",
    ")\n",
    "\n",
    "# then stack the data\n",
    "full_seq = self.data[ix:ix + self.block_size + 1]\n",
    "\n",
    "# usage\n",
    "for x, y in train_loader:  # run batch_size times\n",
    "    logits, loss = model(x, y)\n",
    "```\n",
    "\n",
    "The inf training loop\n",
    "```python\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "train_loader = cycle(DataLoader(train_dataset, batch_size=batch_size))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSamplerDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = int(block_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # single sample\n",
    "        ix = torch.randint(\n",
    "            len(self.data) - self.block_size - 1, (1,)\n",
    "        )\n",
    "        full_seq = self.data[ix:ix + self.block_size + 1]\n",
    "        x = full_seq[:-1]\n",
    "        y = full_seq[1:]\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.block_size\n",
    "\n",
    "\n",
    "train_dataset = TextSamplerDataset(train_data, block_size)\n",
    "val_dataset = TextSamplerDataset(val_data, block_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m<module> -> x:\u001b[0m\n",
      "tensor([[81, 79, 75, 80, 73,  2, 75, 79],\n",
      "        [67, 86, 86, 71, 84, 16,  2,  2],\n",
      "        [80, 77,  2, 52, 71, 67, 84, 70],\n",
      "        [71, 85, 86, 63, 63,  1, 61, 61],\n",
      "        [75, 84,  2, 81, 72,  2, 78, 75],\n",
      "        [74, 71, 71, 84, 78, 71, 67, 70],\n",
      "        [87, 73, 74,  2, 35, 70, 88, 71],\n",
      "        [81, 78, 75, 70, 14,  2, 78, 75],\n",
      "        [75, 80, 73,  2, 41, 84, 67, 69],\n",
      "        [ 2, 69, 67, 80,  2, 67, 82, 82],\n",
      "        [82, 75, 84, 71, 94, 52, 81, 79],\n",
      "        [80, 73, 87, 67, 73, 71, 94, 41],\n",
      "        [ 2, 74, 75, 85,  2, 89, 75, 72],\n",
      "        [75, 77, 75, 82, 71, 70, 75, 67],\n",
      "        [67, 85,  2, 71, 67, 84, 78, 91],\n",
      "        [81, 87, 73, 74,  2, 86, 74, 71],\n",
      "        [61, 61, 37, 74, 71, 79, 75, 69],\n",
      "        [67, 78,  2, 50, 81, 85, 75, 86],\n",
      "        [80, 70,  2, 43, 81, 89, 67,  2],\n",
      "        [63, 11,  1, 12, 61, 61, 19, 26],\n",
      "        [81, 69, 71, 67, 80, 81, 73, 84],\n",
      "        [69, 74, 79, 81, 80, 70, 11,  1],\n",
      "        [75, 78,  2, 69, 67, 16,  2, 26],\n",
      "        [ 2, 86, 81,  2, 86, 74, 71,  2],\n",
      "        [ 2,  9,  9, 61, 61, 41, 71, 79],\n",
      "        [84, 70, 75, 80, 73,  2, 86, 81],\n",
      "        [ 1,  2,  2,  2,  2, 30, 84, 71],\n",
      "        [81, 87, 80, 86, 84, 75, 71, 85],\n",
      "        [84, 81, 86, 81, 86, 91, 82, 71],\n",
      "        [67, 80, 70,  2, 80, 81, 86,  2],\n",
      "        [81, 80, 85, 95, 95,  1,  1, 54],\n",
      "        [80, 86, 81,  2, 86, 71, 84, 84]], device='mps:0')\n",
      "\u001b[93m<module> -> y:\u001b[0m\n",
      "tensor([[79, 75, 80, 73,  2, 75, 79, 82],\n",
      "        [86, 86, 71, 84, 16,  2,  2, 37],\n",
      "        [77,  2, 52, 71, 67, 84, 70, 71],\n",
      "        [85, 86, 63, 63,  1, 61, 61, 76],\n",
      "        [84,  2, 81, 72,  2, 78, 75, 72],\n",
      "        [71, 71, 84, 78, 71, 67, 70, 75],\n",
      "        [73, 74,  2, 35, 70, 88, 71, 80],\n",
      "        [78, 75, 70, 14,  2, 78, 75, 83],\n",
      "        [80, 73,  2, 41, 84, 67, 69, 71],\n",
      "        [69, 67, 80,  2, 67, 82, 82, 71],\n",
      "        [75, 84, 71, 94, 52, 81, 79, 67],\n",
      "        [73, 87, 67, 73, 71, 94, 41, 84],\n",
      "        [74, 75, 85,  2, 89, 75, 72, 71],\n",
      "        [77, 75, 82, 71, 70, 75, 67, 28],\n",
      "        [85,  2, 71, 67, 84, 78, 91,  2],\n",
      "        [87, 73, 74,  2, 86, 74, 71,  2],\n",
      "        [61, 37, 74, 71, 79, 75, 69, 67],\n",
      "        [78,  2, 50, 81, 85, 75, 86, 75],\n",
      "        [70,  2, 43, 81, 89, 67,  2, 53],\n",
      "        [11,  1, 12, 61, 61, 19, 26, 26],\n",
      "        [69, 71, 67, 80, 81, 73, 84, 67],\n",
      "        [74, 79, 81, 80, 70, 11,  1, 94],\n",
      "        [78,  2, 69, 67, 16,  2, 26, 14],\n",
      "        [86, 81,  2, 86, 74, 71,  2, 80],\n",
      "        [ 9,  9, 61, 61, 41, 71, 79, 75],\n",
      "        [70, 75, 80, 73,  2, 86, 81,  2],\n",
      "        [ 2,  2,  2,  2, 30, 84, 71, 88],\n",
      "        [87, 80, 86, 84, 75, 71, 85, 94],\n",
      "        [81, 86, 81, 86, 91, 82, 71, 30],\n",
      "        [80, 70,  2, 80, 81, 86,  2, 86],\n",
      "        [80, 85, 95, 95,  1,  1, 54, 74],\n",
      "        [86, 81,  2, 86, 71, 84, 84, 75]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "x, y = next(dataiter)\n",
    "cprint(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (embedding): Embedding(2102, 2102)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        embedding_dim = vocab_size\n",
    "        # embedding_dim = 128\n",
    "        # each token is represented by a one-hot vector\n",
    "        # directly reads off the logits for the next token from the embedding table\n",
    "        # for example: 24 will reads off the 24th column of the embedding table\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is (batch_size, block_size)\n",
    "        logits = self.embedding(idx)  # B, T, C: (batch_size, block_size, embedding_dim)\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # (batch_size * block_size, embedding_dim)\n",
    "            targets = targets.view(-1)  # (batch_size * block_size)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "If you want to train indefinitely, you can use the following code:\n",
    "\n",
    "```python\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "train_iter = cycle(train_loader)\n",
    "val_iter = cycle(val_loader)\n",
    "```\n",
    "\n",
    "Or \n",
    "```python\n",
    "train_iter = iter(train_loader)\n",
    "val_iter = iter(val_loader)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 8.1311, val loss 8.1381\n",
      "step 99: train loss 8.0976, val loss 8.0863\n"
     ]
    }
   ],
   "source": [
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "train_iter = cycle(train_loader)\n",
    "val_iter = cycle(val_loader)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for _, (x, y) in zip(range(eval_iters), val_iter):\n",
    "                _, loss = model(x, y)\n",
    "                val_losses.append(loss.item())\n",
    "            val_loss = np.mean(val_losses)\n",
    "            \n",
    "            train_losses = []\n",
    "            for _, (x, y) in zip(range(eval_iters), train_loader):\n",
    "                _, loss = model(x, y)\n",
    "                train_losses.append(loss.item())\n",
    "            train_loss = np.mean(train_losses)\n",
    "            \n",
    "            print(f\"step {iter}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "        model.train()\n",
    "\n",
    "    x, y = next(train_iter)  # replace get_batch\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
